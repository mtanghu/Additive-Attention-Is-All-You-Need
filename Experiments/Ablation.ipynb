{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import leap\n", "from leap import LeapForCausalLM, LeapConfig\n", "from lstm import LstmForCausalLM\n", "from transformers import (PreTrainedTokenizerFast, TrainingArguments,\n", "                          Trainer, default_data_collator,\n", "                          GPT2Config, GPT2LMHeadModel)\n", "\n", "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n", "from torch.utils.data import Subset\n", "\n", "# word level tokenizer as per wikitext modeling\n", "from tokenizers import Tokenizer\n", "from tokenizers.models import WordLevel\n", "from tokenizers.pre_tokenizers import Whitespace\n", "from tokenizers.trainers import WordLevelTrainer\n", "\n", "import math\n", "import copy\n", "from itertools import chain\n", "\n", "import logging\n", "logging.disable(logging.INFO)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Found cached dataset wikitext (C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "572d33b0369c497f8b443040c6a51836", "version_major": 2, "version_minor": 0}, "text/plain": ["  0%|          | 0/3 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# globals\n", "raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n", "total_train_tokens = 105268829 # see appendix at the end of notebook\n", "max_num_params = 69308416\n", "param_data_ratio = max_num_params / total_train_tokens\n", "seq_len = 1024\n", "subset_datasets = raw_datasets\n", "\n", "# hyperparameters\n", "training_args = TrainingArguments(\n", "    output_dir = \"./results\",\n", "    logging_strategy = \"steps\",\n", "    evaluation_strategy = \"steps\",\n", "    logging_steps = 2000,\n", "    report_to = \"none\",\n", "    learning_rate = 5e-4,\n", "    lr_scheduler_type = \"cosine\",\n", "    warmup_ratio = .05,\n", "    num_train_epochs = 1,\n", "    per_device_train_batch_size = 2,\n", "    per_device_eval_batch_size = 2,\n", "    max_grad_norm = 1,\n", "    fp16 = True,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# PREPROCESSING"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-a2031eb206d20f87.arrow\n", "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8f3dd2e5d5819fe2.arrow\n", "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-f4668efef485cbea.arrow\n", "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-9004e9678f0a1614.arrow\n", "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-d189dc21c09aa046.arrow\n", "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-50bce12b611e856c.arrow\n"]}], "source": ["# make a word level tokenizer\n", "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n", "tokenizer.pre_tokenizer = Whitespace()\n", "tokenizer.enable_padding(pad_id = 0, pad_token = \"<pad>\")\n", "# no post processing\n", "\n", "# only use vocab size of 8192 for reasonable speed/memory\n", "token_trainer = WordLevelTrainer(vocab_size = 8191, # -1 for pad token\n", "                                 special_tokens = [\"<unk>\"])\n", "\n", "def batch_iterator(batch_size=10000):\n", "    text = raw_datasets[\"train\"]['text']\n", "    for i in range(0, len(text), batch_size):\n", "        yield text[i : i + batch_size]\n", "\n", "tokenizer.train_from_iterator(batch_iterator(),\n", "                              trainer = token_trainer,\n", "                              length = len(raw_datasets[\"train\"][\"text\"]))\n", "tokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer, pad_token = \"<pad>\")\n", "\n", "# tokenized the dataset\n", "def tokenize_function(examples):\n", "    output = tokenizer(examples[\"text\"])\n", "    return output\n", "\n", "# tokenize dataset\n", "tokenized_datasets = raw_datasets.map(\n", "    tokenize_function,\n", "    batched = True,\n", "    remove_columns = \"text\",\n", "    desc = f\"tokenize dataset\",\n", "    load_from_cache_file = True\n", ")\n", "\n", "def group_texts(examples):\n", "    # Concatenate all texts\n", "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n", "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n", "\n", "    # Split by chunks of max_len\n", "    result = {\n", "        k: [t[i : i + seq_len] for i in range(0, total_length, seq_len)]\n", "        for k, t in concatenated_examples.items()\n", "    }\n", "    \n", "    # for language modeling, inputs are labels (they will be shifted inside the model)\n", "    result[\"labels\"] = result[\"input_ids\"].copy()\n", "    \n", "    # pad last block with 0\n", "    last_ids = result[\"input_ids\"][-1]\n", "    diff = seq_len - len(last_ids)\n", "    result[\"input_ids\"][-1] = last_ids + [0 for _ in range(diff)]\n", "    \n", "    # set attention mask to mask out these tokens\n", "    result[\"attention_mask\"][-1] = result[\"attention_mask\"][-1] + [0 for _ in range(diff)]\n", "    \n", "    # set pad labels to -100 so they will be ignored by CrossEntropyLoss\n", "    result[\"labels\"][-1] = result[\"labels\"][-1] + [-100 for _ in range(diff)]\n", "    return result\n", "\n", "lm_dataset = tokenized_datasets.map(\n", "    group_texts,\n", "    batched = True,\n", "    batch_size = 10000,\n", "    desc = f\"Grouping texts in chunks of {seq_len}\",\n", "    load_from_cache_file = True\n", ")\n", "\n", "lm_dataset = lm_dataset.remove_columns([\"token_type_ids\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## helper function"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def subset_data(dataset, num_parameters, param_data_ratio):\n", "    dataset = DatasetDict(copy.deepcopy(dataset))\n", "    subset_num_tokens = num_parameters / param_data_ratio\n", "    \n", "    global seq_len\n", "    num_rows = int(subset_num_tokens) // seq_len\n", "\n", "    training_set = dataset[\"train\"]\n", "    dataset[\"train\"] = Dataset.from_dict(training_set[:num_rows+1])\n", "    \n", "    real_num_tokens = len(dataset[\"train\"]) * seq_len\n", "    print(f'NUMBER OF TOKENS: {real_num_tokens}, with commas {real_num_tokens:,}')\n", "    \n", "    dataset.set_format('pt')\n", "    return dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TRAINING FUNCTION"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_training(hidden_size, n_head = None):\n", "    # calculate number of layers needed based on levine 2020\n", "    n_layer = round((math.log(hidden_size) - 5.039) / 5.55e-2)\n", "    n_layer = max(1, n_layer)\n", "    print(f'Using {n_layer} layers')\n", "    \n", "    config = LeapConfig(\n", "        hidden_size = hidden_size, n_layer = n_layer, n_head = n_head,\n", "        vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n", "        use_local_att = True, window_sizes = None, rescale = 10,\n", "        initializer_range = 1 / hidden_size**.5, hidden_dropout_prob = 0 # no dropout bc one epoch\n", "    )\n", "    model = LeapForCausalLM(config)\n", "\n", "    # get number of parameters\n", "    total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "    \n", "    # get rid of embedding parameters\n", "    token_embeddings = (len(tokenizer) + 1) * hidden_size\n", "    position_embeddings = seq_len * hidden_size\n", "    non_embedding_parameters = total_parameters - token_embeddings - position_embeddings\n", "    print(f'NON EMBEDDING PARAMETERS: {non_embedding_parameters}, with commas {non_embedding_parameters:,}')\n", "\n", "    # subset dataset using global lm_dataset\n", "    global lm_dataset\n", "    subset_datasets = subset_data(lm_dataset, non_embedding_parameters, param_data_ratio)\n", "\n", "    trainer = Trainer(\n", "        model=model,\n", "        args=training_args,\n", "        data_collator=default_data_collator,\n", "        train_dataset=subset_datasets[\"train\"],\n", "        eval_dataset=subset_datasets[\"validation\"],\n", "    )\n", "\n", "    trainer.train()\n", "    \n", "    print(\"\\n===============VALIDATION SET CROSS ENTROPY LOSS EVALUATION===============\\n\")\n", "    print(trainer.evaluate(subset_datasets[\"validation\"]))\n", "\n", "    # save gpu memory\n", "    del trainer\n", "    del model\n", "    del subset_datasets\n", "    torch.cuda.empty_cache()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ablations\n", "\n", "The idea here is instead of changing the source code in LEAP.py over and over again and then running personal experiments, that it makes much more sense to record this stuff in a notebook. Still it seems like a bad option to have to write a bunch of if statements about which little abalation to use. The solution, monkey patching! We will rewrite the LEAP forward function before each abalation test. An example is shown, and this notebook will be annotated"], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## original forward function"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def forward(self, q, f, k, v, attention_mask = None):        \n", "        batch_size, seq_len, hidden_size = v.shape\n", "        \n", "        # reshape for multihead formulation\n", "        q = q.reshape(batch_size, seq_len, self.n_head, self.head_size)\n", "        f = f.reshape(batch_size, seq_len, self.n_head, self.head_size)\n", "        k = k.reshape(batch_size, seq_len, self.n_head, self.head_size)\n", "        v = v.reshape(batch_size, seq_len, self.n_head, self.head_size)\n", "        \n", "        # unparameterized norming of vectors so dot products don't explode (also why it is after reshaping)\n", "        if self.rescale:\n", "            q = self.__real_norm(q)\n", "            f = self.__real_norm(f)\n", "            k = self.__real_norm(k)\n", "            v = self.__real_norm(v)\n", "            \n", "        # dropout regularization (keys don't need dropout as they are always dotted with a dropped out vector)\n", "        q = self.drop(q)\n", "        f = self.drop(f)\n", "        v = self.drop(v)\n", "\n", "        # manual \"matrix dot product\" for speed (in einsum notation \"bshe, bshe->bsh\") with scaling\n", "        focus_logits = (f * k).sum(dim = -1) * self.scaling_factor\n", "        \n", "        # apply dropout to logits so that all tokens will have a chance at getting focus\n", "        focus_logits = self.drop(focus_logits)\n", "        \n", "        # masking out pad tokens\n", "        if attention_mask is not None:\n", "            focus_logits += attention_mask\n", "        \n", "        # manual softmax within cumulative sum\n", "        focus_weights = torch.exp(focus_logits)\n", "        focus_weights = focus_weights.unsqueeze(-1)\n", "        \n", "        # normalization term for softmax\n", "        cumulative_weights = torch.cumsum(focus_weights, dim = 1)\n", "        cumulative_weights = cumulative_weights - self.__window_align(cumulative_weights)\n", "        \n", "        focused_k = self.__w_focus(focus_weights, cumulative_weights, k)\n", "        focused_v = self.__w_focus(focus_weights, cumulative_weights, v)\n", "        \n", "        # querying by measuring dot product alignment (with scaling)\n", "        alignment = torch.sigmoid((q * focused_k).sum(dim = -1) * self.scaling_factor)\n", "        attention = alignment.unsqueeze(-1) * focused_v\n", "        \n", "        # concat heads\n", "        attention = focused_v.reshape(batch_size, seq_len, hidden_size)\n", "        \n", "        return attention"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## monkey patched"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def forward(self, q, f, k, v, attention_mask = None):        \n", "    print(\"hi, we abalate the entirety of LEAP here :)\", end = '\\r')\n", "    return v\n", "\n", "leap.MultiheadLeap.forward = forward"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Using 1 layers\n", "NON EMBEDDING PARAMETERS: 49856, with commas 49,856\n", "NUMBER OF TOKENS: 75776, with commas 75,776\n", "hi, we abalate the entirety of LEAP here :)\r"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [37/37 00:00, Epoch 1/1]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Step</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["hi, we abalate the entirety of LEAP here :)\n", "===============VALIDATION SET CROSS ENTROPY LOSS EVALUATION===============\n", "\n", "hi, we abalate the entirety of LEAP here :)\r"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='106' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [106/106 00:00]\n", "    </div>\n", "    "], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["{'eval_loss': 8.707731246948242, 'eval_runtime': 0.553, 'eval_samples_per_second': 383.364, 'eval_steps_per_second': 191.682, 'epoch': 1.0}\n"]}], "source": ["run_training(64, n_head = 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}