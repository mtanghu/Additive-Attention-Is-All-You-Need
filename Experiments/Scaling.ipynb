{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from leap import LeapForCausalLM, LeapConfig\n",
    "from lstm import LstmForCausalLM\n",
    "from transformers import (PreTrainedTokenizerFast, TrainingArguments,\n",
    "                          Trainer, default_data_collator,\n",
    "                          GPT2Config, GPT2LMHeadModel)\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# word level tokenizer as per wikitext modeling\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from itertools import chain\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5675743b4044cf87fd675daf1dc578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# globals\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "total_train_tokens = 105268829 # see appendix at the end of notebook\n",
    "max_num_params = 69308416\n",
    "param_data_ratio = max_num_params / total_train_tokens\n",
    "seq_len = 1024\n",
    "subset_datasets = raw_datasets\n",
    "\n",
    "# hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 5e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    max_grad_norm = 1,\n",
    "    fp16 = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-a2031eb206d20f87.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8f3dd2e5d5819fe2.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-f4668efef485cbea.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-9004e9678f0a1614.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-d189dc21c09aa046.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-50bce12b611e856c.arrow\n"
     ]
    }
   ],
   "source": [
    "# make a word level tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(pad_id = 0, pad_token = \"<pad>\")\n",
    "# no post processing\n",
    "\n",
    "# only use vocab size of 8192 for reasonable speed/memory\n",
    "token_trainer = WordLevelTrainer(vocab_size = 8191, # -1 for pad token\n",
    "                                 special_tokens = [\"<unk>\"])\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    text = raw_datasets[\"train\"]['text']\n",
    "    for i in range(0, len(text), batch_size):\n",
    "        yield text[i : i + batch_size]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(),\n",
    "                              trainer = token_trainer,\n",
    "                              length = len(raw_datasets[\"train\"][\"text\"]))\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer, pad_token = \"<pad>\")\n",
    "\n",
    "# tokenized the dataset\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[\"text\"])\n",
    "    return output\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched = True,\n",
    "    remove_columns = \"text\",\n",
    "    desc = f\"tokenize dataset\",\n",
    "    load_from_cache_file = True\n",
    ")\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + seq_len] for i in range(0, total_length, seq_len)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # for language modeling, inputs are labels (they will be shifted inside the model)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    # pad last block with 0\n",
    "    last_ids = result[\"input_ids\"][-1]\n",
    "    diff = seq_len - len(last_ids)\n",
    "    result[\"input_ids\"][-1] = last_ids + [0 for _ in range(diff)]\n",
    "    \n",
    "    # set attention mask to mask out these tokens\n",
    "    result[\"attention_mask\"][-1] = result[\"attention_mask\"][-1] + [0 for _ in range(diff)]\n",
    "    \n",
    "    # set pad labels to -100 so they will be ignored by CrossEntropyLoss\n",
    "    result[\"labels\"][-1] = result[\"labels\"][-1] + [-100 for _ in range(diff)]\n",
    "    return result\n",
    "\n",
    "# set globally block size for group texts function\n",
    "lm_dataset = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched = True,\n",
    "    batch_size = 10000,\n",
    "    desc = f\"Grouping texts in chunks of {seq_len}\",\n",
    "    load_from_cache_file = True\n",
    ")\n",
    "\n",
    "lm_dataset = lm_dataset.remove_columns([\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data(dataset, num_parameters, param_data_ratio):\n",
    "    dataset = DatasetDict(copy.deepcopy(dataset))\n",
    "    subset_num_tokens = num_parameters / param_data_ratio\n",
    "    \n",
    "    global seq_len\n",
    "    num_rows = int(subset_num_tokens) // seq_len\n",
    "\n",
    "    training_set = dataset[\"train\"]\n",
    "    dataset[\"train\"] = Dataset.from_dict(training_set[:num_rows+1])\n",
    "    \n",
    "    real_num_tokens = len(dataset[\"train\"]) * seq_len\n",
    "    print(f'NUMBER OF TOKENS: {real_num_tokens}, with commas {real_num_tokens:,}')\n",
    "    \n",
    "    dataset.set_format('pt')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(hidden_size, n_head = None, gpt = False, rnn = False):\n",
    "    # calculate number of layers needed based on levine 2020\n",
    "    n_layer = round((math.log(hidden_size) - 5.039) / 5.55e-2)\n",
    "    n_layer = max(1, n_layer)\n",
    "    print(f'Using {n_layer} layers')\n",
    "    \n",
    "    # get number of parameters\n",
    "    if gpt is True:\n",
    "        config = GPT2Config(\n",
    "            n_embd = hidden_size, n_layer = n_layer,\n",
    "            n_head = 1, vocab_size = 0, n_positions = 0\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    elif rnn is True:\n",
    "        model = LstmForCausalLM(\n",
    "            hidden_size = hidden_size,\n",
    "            n_layer = n_layer,\n",
    "            vocab_size = 0\n",
    "        )\n",
    "    else:\n",
    "        config = LeapConfig(\n",
    "            hidden_size = hidden_size, n_layer = n_layer,\n",
    "            n_head = n_head, vocab_size = 0, n_positions = 0\n",
    "        )\n",
    "        model = LeapForCausalLM(config)\n",
    "\n",
    "    non_embedding_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'NON EMBEDDING PARAMETERS: {non_embedding_parameters}, with commas {non_embedding_parameters:,}')\n",
    "\n",
    "    # subset dataset using global lm_dataset\n",
    "    global lm_dataset\n",
    "    subset_datasets = subset_data(lm_dataset, non_embedding_parameters, param_data_ratio)\n",
    "\n",
    "    if gpt is True:\n",
    "        config = GPT2Config(\n",
    "            n_embd = hidden_size, n_layer = n_layer, n_head = n_head,\n",
    "            vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n",
    "            initializer_range = 1 / hidden_size**.5,\n",
    "            resid_pdrop = 0, embd_pdrop = 0, attn_pdrop = 0 # no dropout bc one epoch\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    elif rnn is True:\n",
    "        model = LstmForCausalLM(\n",
    "            hidden_size = hidden_size,\n",
    "            n_layer = n_layer,\n",
    "            vocab_size = len(tokenizer) + 1,\n",
    "            hidden_dropout_prob = 0\n",
    "        )\n",
    "    else:\n",
    "        config = LeapConfig(\n",
    "            hidden_size = hidden_size, n_layer = n_layer, n_head = n_head,\n",
    "            vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n",
    "            use_local_att = True, window_sizes = None, rescale = 10,\n",
    "            initializer_range = 1 / hidden_size**.5, hidden_dropout_prob = 0 # no dropout bc one epoch\n",
    "        )\n",
    "        model = LeapForCausalLM(config)\n",
    "        \n",
    "    model = model.cuda()\n",
    "    input_ids = subset_datasets[\"train\"][0][\"input_ids\"].reshape(1, -1).cuda()\n",
    "    print(profile(model, inputs = (input_ids, )))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator,\n",
    "        train_dataset=subset_datasets[\"train\"],\n",
    "        eval_dataset=subset_datasets[\"validation\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\\n\")\n",
    "    print(f'Numeric form: {int(trainer.state.total_flos)}\\nHuman Readable: {int(trainer.state.total_flos):,}')\n",
    "\n",
    "    print(\"\\n===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\\n\")\n",
    "    print(trainer.evaluate(subset_datasets[\"test\"]))\n",
    "\n",
    "    # save gpu memory\n",
    "    del trainer\n",
    "    del model\n",
    "    del subset_datasets\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 49856, with commas 49,856\n",
      "NUMBER OF TOKENS: 75776, with commas 75,776\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "(587988992.0, 574144.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 22667329536\n",
      "Human Readable: 22,667,329,536\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.310734748840332, 'eval_runtime': 0.6468, 'eval_samples_per_second': 367.96, 'eval_steps_per_second': 183.98, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 50112, with commas 50,112\n",
      "NUMBER OF TOKENS: 76800, with commas 76,800\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "(537657344.0, 524672.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 23091609600\n",
      "Human Readable: 23,091,609,600\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.099257469177246, 'eval_runtime': 0.6025, 'eval_samples_per_second': 395.026, 'eval_steps_per_second': 197.513, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1, gpt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 TRAINING\n",
    "Each run is done seperately in it's own cell just for easy viewing of logs and in case something goes wrong (OOM errors or training issues). Note that the learning rate had to be lowered from 1e-3 to 5e-4 because gpt2 wasn't converging on the largest test and all tests redone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 50112, with commas 50,112\n",
      "NUMBER OF TOKENS: 76800, with commas 76,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 23091609600\n",
      "Human Readable: 23,091,609,600\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.926460266113281, 'eval_runtime': 0.8337, 'eval_samples_per_second': 285.461, 'eval_steps_per_second': 142.731, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 198528, with commas 198,528\n",
      "NUMBER OF TOKENS: 302080, with commas 302,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='148' max='148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [148/148 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 359828029440\n",
      "Human Readable: 359,828,029,440\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.967508792877197, 'eval_runtime': 0.7859, 'eval_samples_per_second': 302.852, 'eval_steps_per_second': 151.426, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, n_head = 2, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 309600, with commas 309,600\n",
      "NUMBER OF TOKENS: 471040, with commas 471,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 875003904000\n",
      "Human Readable: 875,003,904,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.784952163696289, 'eval_runtime': 0.8076, 'eval_samples_per_second': 294.718, 'eval_steps_per_second': 147.359, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, n_head = 2, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1779840, with commas 1,779,840\n",
      "NUMBER OF TOKENS: 2703360, with commas 2,703,360\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 00:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.741000</td>\n",
       "      <td>5.359797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>5.165725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 28869289574400\n",
      "Human Readable: 28,869,289,574,400\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.0926594734191895, 'eval_runtime': 1.4663, 'eval_samples_per_second': 162.308, 'eval_steps_per_second': 81.154, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, n_head = 3, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 layers\n",
      "NON EMBEDDING PARAMETERS: 4235616, with commas 4,235,616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 6433792, with commas 6,433,792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3142' max='3142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3142/3142 04:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.781400</td>\n",
       "      <td>5.319905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.121200</td>\n",
       "      <td>5.079702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.947100</td>\n",
       "      <td>4.931954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.847300</td>\n",
       "      <td>4.858215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.763500</td>\n",
       "      <td>4.801550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.746900</td>\n",
       "      <td>4.782231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 163506434015232\n",
      "Human Readable: 163,506,434,015,232\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.705209255218506, 'eval_runtime': 2.5793, 'eval_samples_per_second': 92.273, 'eval_steps_per_second': 46.136, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 224, n_head = 4, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n",
      "NON EMBEDDING PARAMETERS: 7108352, with commas 7,108,352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 10797056, with commas 10,797,056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5272' max='5272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5272/5272 08:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.822100</td>\n",
       "      <td>5.338395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.126500</td>\n",
       "      <td>5.067064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.942500</td>\n",
       "      <td>4.926499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.855000</td>\n",
       "      <td>4.837709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.754700</td>\n",
       "      <td>4.740122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.661600</td>\n",
       "      <td>4.662832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.586700</td>\n",
       "      <td>4.588413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.519800</td>\n",
       "      <td>4.530913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.477500</td>\n",
       "      <td>4.493793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.439200</td>\n",
       "      <td>4.481374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 460495647670272\n",
      "Human Readable: 460,495,647,670,272\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.404869079589844, 'eval_runtime': 3.3211, 'eval_samples_per_second': 71.664, 'eval_steps_per_second': 35.832, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 256, n_head = 4, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 16029120, with commas 16,029,120\n",
      "NUMBER OF TOKENS: 24346624, with commas 24,346,624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11888' max='11888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11888/11888 32:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.898800</td>\n",
       "      <td>5.385432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.159900</td>\n",
       "      <td>5.114223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.979000</td>\n",
       "      <td>4.946620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.867000</td>\n",
       "      <td>4.849581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.781000</td>\n",
       "      <td>4.769125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.677300</td>\n",
       "      <td>4.652411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.596700</td>\n",
       "      <td>4.588578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.499500</td>\n",
       "      <td>4.489614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.405500</td>\n",
       "      <td>4.381318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.347800</td>\n",
       "      <td>4.297613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.232300</td>\n",
       "      <td>4.219705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.155500</td>\n",
       "      <td>4.140476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.100800</td>\n",
       "      <td>4.082276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.055000</td>\n",
       "      <td>4.036297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.010100</td>\n",
       "      <td>3.995474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.960100</td>\n",
       "      <td>3.957351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.929100</td>\n",
       "      <td>3.925721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.896000</td>\n",
       "      <td>3.897912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.869900</td>\n",
       "      <td>3.876129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.862900</td>\n",
       "      <td>3.860160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.837800</td>\n",
       "      <td>3.847438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.851500</td>\n",
       "      <td>3.841327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.856600</td>\n",
       "      <td>3.838645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 2341529746145280\n",
      "Human Readable: 2,341,529,746,145,280\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.793569326400757, 'eval_runtime': 5.4882, 'eval_samples_per_second': 43.365, 'eval_steps_per_second': 21.683, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 320, n_head = 5, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 45872064, with commas 45,872,064\n",
      "NUMBER OF TOKENS: 69672960, with commas 69,672,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34020' max='34020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34020/34020 2:56:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.016700</td>\n",
       "      <td>5.460122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.256400</td>\n",
       "      <td>5.174232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.044100</td>\n",
       "      <td>5.035386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.942300</td>\n",
       "      <td>4.935595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.835700</td>\n",
       "      <td>4.830333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.743200</td>\n",
       "      <td>4.746832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.713500</td>\n",
       "      <td>4.696311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.652400</td>\n",
       "      <td>4.649318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.625700</td>\n",
       "      <td>4.603781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.552100</td>\n",
       "      <td>4.538907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.487800</td>\n",
       "      <td>4.493001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.440400</td>\n",
       "      <td>4.407402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.370100</td>\n",
       "      <td>4.325207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.293700</td>\n",
       "      <td>4.242820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.213200</td>\n",
       "      <td>4.174459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.130400</td>\n",
       "      <td>4.114735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.081100</td>\n",
       "      <td>4.051942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.023300</td>\n",
       "      <td>4.013743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.976500</td>\n",
       "      <td>3.958775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.947000</td>\n",
       "      <td>3.925998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.887400</td>\n",
       "      <td>3.880393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.891600</td>\n",
       "      <td>3.842133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.834000</td>\n",
       "      <td>3.816673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.813800</td>\n",
       "      <td>3.784295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.784700</td>\n",
       "      <td>3.761065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.746200</td>\n",
       "      <td>3.733501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.719600</td>\n",
       "      <td>3.709214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.695100</td>\n",
       "      <td>3.680710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.665900</td>\n",
       "      <td>3.663071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.659700</td>\n",
       "      <td>3.634368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.630300</td>\n",
       "      <td>3.616374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.623100</td>\n",
       "      <td>3.598446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.598000</td>\n",
       "      <td>3.589231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.583700</td>\n",
       "      <td>3.565789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.564300</td>\n",
       "      <td>3.546044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.559600</td>\n",
       "      <td>3.530474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.538300</td>\n",
       "      <td>3.512211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.507300</td>\n",
       "      <td>3.501645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.520600</td>\n",
       "      <td>3.490513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.503100</td>\n",
       "      <td>3.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.480600</td>\n",
       "      <td>3.463332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.478200</td>\n",
       "      <td>3.449253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.467400</td>\n",
       "      <td>3.441566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.477900</td>\n",
       "      <td>3.427445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.441300</td>\n",
       "      <td>3.416696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.430500</td>\n",
       "      <td>3.403525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.422200</td>\n",
       "      <td>3.396128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.409700</td>\n",
       "      <td>3.388842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.403600</td>\n",
       "      <td>3.378385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.386200</td>\n",
       "      <td>3.371457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.406200</td>\n",
       "      <td>3.362996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.394100</td>\n",
       "      <td>3.354327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.374800</td>\n",
       "      <td>3.348065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.352100</td>\n",
       "      <td>3.341830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.380800</td>\n",
       "      <td>3.336872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.362600</td>\n",
       "      <td>3.332177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.347200</td>\n",
       "      <td>3.327751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.362500</td>\n",
       "      <td>3.323535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.362000</td>\n",
       "      <td>3.320704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.347200</td>\n",
       "      <td>3.317785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.346200</td>\n",
       "      <td>3.314834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.363500</td>\n",
       "      <td>3.312718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.344500</td>\n",
       "      <td>3.311607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.321700</td>\n",
       "      <td>3.310348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.321500</td>\n",
       "      <td>3.309519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.331100</td>\n",
       "      <td>3.309149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.333400</td>\n",
       "      <td>3.309032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.323300</td>\n",
       "      <td>3.309001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 19176254881136640\n",
      "Human Readable: 19,176,254,881,136,640\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.298440456390381, 'eval_runtime': 10.8555, 'eval_samples_per_second': 21.924, 'eval_steps_per_second': 10.962, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 448, n_head = 7, gpt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEAP TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 49856, with commas 49,856\n",
      "NUMBER OF TOKENS: 75776, with commas 75,776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 22667329536\n",
      "Human Readable: 22,667,329,536\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.310734748840332, 'eval_runtime': 0.8748, 'eval_samples_per_second': 272.048, 'eval_steps_per_second': 136.024, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 198016, with commas 198,016\n",
      "NUMBER OF TOKENS: 301056, with commas 301,056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 357683429376\n",
      "Human Readable: 357,683,429,376\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0050950050354, 'eval_runtime': 0.8547, 'eval_samples_per_second': 278.469, 'eval_steps_per_second': 139.235, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, n_head = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 308960, with commas 308,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\desktop\\leap\\src\\leap\\LEAP.py:200: UserWarning: Using a hidden_size-to-head ratio of greater than 64 is not ideal as LEAP uses a simplified form of attention that relies on having many heads\n",
      "  warnings.warn(\"Using a hidden_size-to-head ratio of greater than 64 is not ideal as\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 470016, with commas 470,016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 871296860160\n",
      "Human Readable: 871,296,860,160\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.812751293182373, 'eval_runtime': 0.8401, 'eval_samples_per_second': 283.285, 'eval_steps_per_second': 141.643, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, n_head = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1776768, with commas 1,776,768\n",
      "NUMBER OF TOKENS: 2699264, with commas 2,699,264\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1318' max='1318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1318/1318 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.775900</td>\n",
       "      <td>5.333490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.124500</td>\n",
       "      <td>5.118966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 28775795392512\n",
      "Human Readable: 28,775,795,392,512\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.051537990570068, 'eval_runtime': 1.5267, 'eval_samples_per_second': 155.895, 'eval_steps_per_second': 77.948, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, n_head = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 layers\n",
      "NON EMBEDDING PARAMETERS: 4229344, with commas 4,229,344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 6424576, with commas 6,424,576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3137' max='3137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3137/3137 03:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.823700</td>\n",
       "      <td>5.271842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.044900</td>\n",
       "      <td>4.985031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.833200</td>\n",
       "      <td>4.821365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.697600</td>\n",
       "      <td>4.697044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.600900</td>\n",
       "      <td>4.632804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.559500</td>\n",
       "      <td>4.612202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 163030451748864\n",
      "Human Readable: 163,030,451,748,864\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.546367645263672, 'eval_runtime': 2.2102, 'eval_samples_per_second': 107.682, 'eval_steps_per_second': 53.841, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 224, n_head = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n",
      "NON EMBEDDING PARAMETERS: 7099136, with commas 7,099,136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 10782720, with commas 10,782,720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5265' max='5265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5265/5265 07:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.886600</td>\n",
       "      <td>5.288266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.064200</td>\n",
       "      <td>4.950306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.806300</td>\n",
       "      <td>4.747504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.630400</td>\n",
       "      <td>4.598880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.516400</td>\n",
       "      <td>4.503559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.427400</td>\n",
       "      <td>4.409701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.361600</td>\n",
       "      <td>4.353199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.299800</td>\n",
       "      <td>4.313859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.271300</td>\n",
       "      <td>4.288555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.246400</td>\n",
       "      <td>4.280110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 459287974379520\n",
      "Human Readable: 459,287,974,379,520\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.207425117492676, 'eval_runtime': 2.5841, 'eval_samples_per_second': 92.102, 'eval_steps_per_second': 46.051, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 256, n_head = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n",
      "NON EMBEDDING PARAMETERS: 16012480, with commas 16,012,480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 24321024, with commas 24,321,024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11876' max='11876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11876/11876 24:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.010300</td>\n",
       "      <td>5.372142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.116400</td>\n",
       "      <td>4.999618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.821300</td>\n",
       "      <td>4.766925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.664200</td>\n",
       "      <td>4.612965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.522200</td>\n",
       "      <td>4.486940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.433600</td>\n",
       "      <td>4.407304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.325400</td>\n",
       "      <td>4.310812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.257900</td>\n",
       "      <td>4.242762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.190100</td>\n",
       "      <td>4.178706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.134100</td>\n",
       "      <td>4.119658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.076000</td>\n",
       "      <td>4.066662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.028300</td>\n",
       "      <td>4.019446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.997600</td>\n",
       "      <td>3.984490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.963000</td>\n",
       "      <td>3.954651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.932000</td>\n",
       "      <td>3.924968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.898900</td>\n",
       "      <td>3.897268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.872100</td>\n",
       "      <td>3.874192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.870100</td>\n",
       "      <td>3.855009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.855100</td>\n",
       "      <td>3.840235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.830400</td>\n",
       "      <td>3.828966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.823900</td>\n",
       "      <td>3.820287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.803600</td>\n",
       "      <td>3.816359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.806600</td>\n",
       "      <td>3.814691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 2336639462277120\n",
      "Human Readable: 2,336,639,462,277,120\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7722272872924805, 'eval_runtime': 3.6228, 'eval_samples_per_second': 65.696, 'eval_steps_per_second': 32.848, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 320, n_head = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 45838016, with commas 45,838,016\n",
      "NUMBER OF TOKENS: 69621760, with commas 69,621,760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33995' max='33995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33995/33995 2:04:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.146000</td>\n",
       "      <td>5.491266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.221700</td>\n",
       "      <td>5.120208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.936700</td>\n",
       "      <td>4.894434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.767500</td>\n",
       "      <td>4.707723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.611200</td>\n",
       "      <td>4.546397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.457300</td>\n",
       "      <td>4.428238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.362800</td>\n",
       "      <td>4.333829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.279700</td>\n",
       "      <td>4.243402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.189100</td>\n",
       "      <td>4.160590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.135300</td>\n",
       "      <td>4.126106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.079600</td>\n",
       "      <td>4.056299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.044000</td>\n",
       "      <td>4.011022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.993700</td>\n",
       "      <td>3.965852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.960500</td>\n",
       "      <td>3.945307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.915000</td>\n",
       "      <td>3.886480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.872900</td>\n",
       "      <td>3.859644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.850400</td>\n",
       "      <td>3.828412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.846200</td>\n",
       "      <td>3.806767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.802200</td>\n",
       "      <td>3.789041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.810900</td>\n",
       "      <td>3.757689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.764400</td>\n",
       "      <td>3.733998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.768100</td>\n",
       "      <td>3.715639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.714700</td>\n",
       "      <td>3.700525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.697000</td>\n",
       "      <td>3.687514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.678900</td>\n",
       "      <td>3.667403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.654800</td>\n",
       "      <td>3.649836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.663100</td>\n",
       "      <td>3.631603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.632300</td>\n",
       "      <td>3.614221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.623900</td>\n",
       "      <td>3.598251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.585700</td>\n",
       "      <td>3.583209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.584500</td>\n",
       "      <td>3.570940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.569100</td>\n",
       "      <td>3.554209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.559700</td>\n",
       "      <td>3.538634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.528700</td>\n",
       "      <td>3.526557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.541300</td>\n",
       "      <td>3.514261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.523600</td>\n",
       "      <td>3.504481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.504000</td>\n",
       "      <td>3.494856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.490600</td>\n",
       "      <td>3.484246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.464100</td>\n",
       "      <td>3.474551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.479100</td>\n",
       "      <td>3.462770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.472900</td>\n",
       "      <td>3.452663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>3.440556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.456400</td>\n",
       "      <td>3.429471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.444800</td>\n",
       "      <td>3.417194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.446300</td>\n",
       "      <td>3.411345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.412800</td>\n",
       "      <td>3.403342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.426100</td>\n",
       "      <td>3.394093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.392800</td>\n",
       "      <td>3.382164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.396400</td>\n",
       "      <td>3.375419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.381200</td>\n",
       "      <td>3.369220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.389700</td>\n",
       "      <td>3.362674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.359300</td>\n",
       "      <td>3.356316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.366400</td>\n",
       "      <td>3.350354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.366900</td>\n",
       "      <td>3.344783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.366300</td>\n",
       "      <td>3.340737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.366000</td>\n",
       "      <td>3.335429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.364700</td>\n",
       "      <td>3.332757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.341200</td>\n",
       "      <td>3.329014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.339800</td>\n",
       "      <td>3.325879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.352300</td>\n",
       "      <td>3.323813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.343000</td>\n",
       "      <td>3.320973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.354900</td>\n",
       "      <td>3.320106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.335300</td>\n",
       "      <td>3.318743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.356700</td>\n",
       "      <td>3.317573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.344200</td>\n",
       "      <td>3.317212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.344300</td>\n",
       "      <td>3.316771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.344100</td>\n",
       "      <td>3.316542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 19147940092968960\n",
      "Human Readable: 19,147,940,092,968,960\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3028292655944824, 'eval_runtime': 6.8582, 'eval_samples_per_second': 34.703, 'eval_steps_per_second': 17.351, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 448, n_head = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 22 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 69308416, with commas 69,308,416\n",
      "NUMBER OF TOKENS: 102674432, with commas 102,674,432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50134' max='50134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50134/50134 3:55:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.246000</td>\n",
       "      <td>5.543831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.281400</td>\n",
       "      <td>5.144049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.999100</td>\n",
       "      <td>4.921033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.794900</td>\n",
       "      <td>4.778766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.669300</td>\n",
       "      <td>4.640341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.551800</td>\n",
       "      <td>4.499169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.429400</td>\n",
       "      <td>4.391254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.330600</td>\n",
       "      <td>4.297758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.251100</td>\n",
       "      <td>4.213497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>4.162189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.123100</td>\n",
       "      <td>4.088035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.051800</td>\n",
       "      <td>4.020055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.005800</td>\n",
       "      <td>3.970910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.948300</td>\n",
       "      <td>3.944308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.908300</td>\n",
       "      <td>3.901265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.899500</td>\n",
       "      <td>3.882526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>3.846865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.824200</td>\n",
       "      <td>3.807373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.811200</td>\n",
       "      <td>3.795446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.813300</td>\n",
       "      <td>3.760978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.756100</td>\n",
       "      <td>3.754450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.745400</td>\n",
       "      <td>3.728710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.735000</td>\n",
       "      <td>3.709111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.701400</td>\n",
       "      <td>3.684049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.670583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.669300</td>\n",
       "      <td>3.650347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.664300</td>\n",
       "      <td>3.638619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.623200</td>\n",
       "      <td>3.613945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.626100</td>\n",
       "      <td>3.606994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.606500</td>\n",
       "      <td>3.592207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.581200</td>\n",
       "      <td>3.566401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.575200</td>\n",
       "      <td>3.554549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.559000</td>\n",
       "      <td>3.545115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.552600</td>\n",
       "      <td>3.535928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.573100</td>\n",
       "      <td>3.526052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.551900</td>\n",
       "      <td>3.515987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.538200</td>\n",
       "      <td>3.506615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.523000</td>\n",
       "      <td>3.497039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.504900</td>\n",
       "      <td>3.482765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.498300</td>\n",
       "      <td>3.480206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.496600</td>\n",
       "      <td>3.464776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.480700</td>\n",
       "      <td>3.457822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.481700</td>\n",
       "      <td>3.436278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.471000</td>\n",
       "      <td>3.429375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.451300</td>\n",
       "      <td>3.422621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.440400</td>\n",
       "      <td>3.409058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.412700</td>\n",
       "      <td>3.397790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.414100</td>\n",
       "      <td>3.397361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.424000</td>\n",
       "      <td>3.389241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.414200</td>\n",
       "      <td>3.375584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.383200</td>\n",
       "      <td>3.365324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.397200</td>\n",
       "      <td>3.357483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.374600</td>\n",
       "      <td>3.356117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.373100</td>\n",
       "      <td>3.343152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.365100</td>\n",
       "      <td>3.337217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.366700</td>\n",
       "      <td>3.330922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.362000</td>\n",
       "      <td>3.319662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.344800</td>\n",
       "      <td>3.312297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.321600</td>\n",
       "      <td>3.306594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.340300</td>\n",
       "      <td>3.300438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.306800</td>\n",
       "      <td>3.292896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.330500</td>\n",
       "      <td>3.285520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.305900</td>\n",
       "      <td>3.278702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.301700</td>\n",
       "      <td>3.271421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.296400</td>\n",
       "      <td>3.267629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.293900</td>\n",
       "      <td>3.263016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.288800</td>\n",
       "      <td>3.256609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.281200</td>\n",
       "      <td>3.247588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.280800</td>\n",
       "      <td>3.241036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.271100</td>\n",
       "      <td>3.234436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.267500</td>\n",
       "      <td>3.230312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.256100</td>\n",
       "      <td>3.225216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.251700</td>\n",
       "      <td>3.222224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.249500</td>\n",
       "      <td>3.219542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.249600</td>\n",
       "      <td>3.213490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.240600</td>\n",
       "      <td>3.208464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.241200</td>\n",
       "      <td>3.205687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.231100</td>\n",
       "      <td>3.201242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.241200</td>\n",
       "      <td>3.197560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.237300</td>\n",
       "      <td>3.196097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.212400</td>\n",
       "      <td>3.192892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.215200</td>\n",
       "      <td>3.188431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.230500</td>\n",
       "      <td>3.185932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.211100</td>\n",
       "      <td>3.183945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.228100</td>\n",
       "      <td>3.182251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.225800</td>\n",
       "      <td>3.179762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.210600</td>\n",
       "      <td>3.177904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.187400</td>\n",
       "      <td>3.175651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.210200</td>\n",
       "      <td>3.174347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.199300</td>\n",
       "      <td>3.173142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.204700</td>\n",
       "      <td>3.171705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.219800</td>\n",
       "      <td>3.171021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.192100</td>\n",
       "      <td>3.170114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.194700</td>\n",
       "      <td>3.169352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.201700</td>\n",
       "      <td>3.168695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.212200</td>\n",
       "      <td>3.168495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.206700</td>\n",
       "      <td>3.168223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.194100</td>\n",
       "      <td>3.168068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>3.168015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.220400</td>\n",
       "      <td>3.168013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 42697213473718272\n",
      "Human Readable: 42,697,213,473,718,272\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1646018028259277, 'eval_runtime': 8.8601, 'eval_samples_per_second': 26.862, 'eval_steps_per_second': 13.431, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 512, n_head = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "gpt2_param = np.array([\n",
    "    50112,\n",
    "    198528,\n",
    "    309600,\n",
    "    1779840,\n",
    "    4235616,\n",
    "    7108352,\n",
    "    16029120,\n",
    "    45872064,\n",
    "], dtype = 'int64')\n",
    "\n",
    "gpt2_flos = np.array([\n",
    "    23091609600,\n",
    "    359828029440,\n",
    "    875003904000,\n",
    "    28869289574400,\n",
    "    163506434015232,\n",
    "    460495647670272,\n",
    "    2341529746145280,\n",
    "    19176254881136640,\n",
    "], dtype = 'int64')\n",
    "\n",
    "gpt2_y = np.array([\n",
    "    7.926,\n",
    "    5.968,\n",
    "    5.785,\n",
    "    5.093,\n",
    "    4.705,\n",
    "    4.405,\n",
    "    3.794,\n",
    "    3.298,\n",
    "])\n",
    "\n",
    "leap_param = np.array([\n",
    "    49856,\n",
    "    198016,\n",
    "    308960,\n",
    "    1776768,\n",
    "    4229344,\n",
    "    7099136,\n",
    "    16012480,\n",
    "    45838016,\n",
    "    69308416,\n",
    "], dtype = 'int64')\n",
    "\n",
    "\n",
    "leap_flos = np.array([\n",
    "    22667329536,\n",
    "    357683429376,\n",
    "    871296860160,\n",
    "    28775795392512,\n",
    "    163030451748864,\n",
    "    459287974379520,\n",
    "    2336639462277120,\n",
    "    19147940092968960,\n",
    "    42697213473718272,\n",
    "], dtype = 'int64')\n",
    "\n",
    "\n",
    "leap_y = np.array([\n",
    "    8.311,\n",
    "    6.005,\n",
    "    5.813,\n",
    "    5.052,\n",
    "    4.546,\n",
    "    4.207,\n",
    "    3.772,\n",
    "    3.303,\n",
    "    3.165,\n",
    "])\n",
    "\n",
    "lstm_x = np.array([\n",
    "])\n",
    "\n",
    "lstm_y = np.array([\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-97-5aa542954680>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n",
      "<ipython-input-97-5aa542954680>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n",
      "<ipython-input-97-5aa542954680>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 2.0882097755194463e+25 0.05868012087685672\n",
      "GPT2 1524855591762.3096 0.11729084755791729\n",
      "LEAP 477368224733.0826 0.12782488241964074\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def powerlaw(n, n_c, a):\n",
    "    return (n_c / n)**a\n",
    "\n",
    "\n",
    "\n",
    "# for comptute\n",
    "plt.subplot(1, 2, 1)\n",
    "x = 10**np.arange(start = 9.4, stop = 17.7, step = .1, dtype = 'float64')\n",
    "plt.loglog()\n",
    "plt.yticks([3, 4, 5, 6, 7, 8, 9], labels = [3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "#plt.plot(gpt2_flos, gpt2_y, \".-\", color = 'C0', label = \"GPT2 linear interpolation\", markersize = 10)\n",
    "plt.scatter(x = gpt2_flos, y = gpt2_y, s = 100, alpha = .6, color = 'blue')\n",
    "(n_c, a), _ = curve_fit(powerlaw, gpt2_flos, gpt2_y, maxfev=10000, p0 = np.array([9e30, .05]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'GPT2 powerlaw alpha = {-a:.3}', linestyle = '--', color = 'blue')\n",
    "print(\"GPT2\", n_c, a)\n",
    "\n",
    "\n",
    "plt.xlabel(\"FLOPs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "# for parameters\n",
    "plt.subplot(1, 2, 2)\n",
    "x = 10**np.arange(start = 4.3, stop = 8.5, step = .1, dtype = 'float64')\n",
    "plt.loglog()\n",
    "plt.yticks([2, 3, 4, 5, 6, 7, 8, 9], labels = [2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "plt.scatter(x = gpt2_param, y = gpt2_y, s = 100, alpha = .6, color = 'blue')\n",
    "(n_c, a), _ = curve_fit(powerlaw, gpt2_param, gpt2_y, maxfev=10000, p0 = np.array([9e13, .076]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'GPT2 powerlaw alpha = {-a:.3}', linestyle = '--', color = 'blue')\n",
    "print(\"GPT2\", n_c, a)\n",
    "\n",
    "plt.scatter(x = leap_param, y = leap_y, s = 100, alpha = .6, color = 'C1')\n",
    "(n_c, a), _ = curve_fit(powerlaw, leap_param, leap_y, maxfev=10000, p0 = np.array([9e13, .076]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'LEAP powerlaw alpha = {-a:.2}', linestyle = '--', color = 'C1')\n",
    "print(\"LEAP\", n_c, a)\n",
    "\n",
    "plt.xlabel(\"Non-Embedding Parameters\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('powerlaws.png', dpi = 150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105268829"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# to count tokens, comes from https://huggingface.co/docs/tokenizers/components\n",
    "whitespace_regex = re.compile(\"\\w+|[^\\w\\s]+\")\n",
    "\n",
    "# get number of tokens\n",
    "total_tokens = 0\n",
    "for row in raw_datasets[\"train\"][\"text\"]:\n",
    "    total_tokens += len((whitespace_regex.split(row)))\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 33408, with commas 33,408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 369664, with commas 369,664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='181' max='181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [181/181 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74098409472\n",
      "Human Readable: 74,098,409,472\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.131314277648926, 'eval_runtime': 3.2103, 'eval_samples_per_second': 74.137, 'eval_steps_per_second': 37.068, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 74688, with commas 74,688\n",
      "NUMBER OF TOKENS: 670720, with commas 670,720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='328' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [328/328 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 300568412160\n",
      "Human Readable: 300,568,412,160\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.494235038757324, 'eval_runtime': 3.5429, 'eval_samples_per_second': 67.177, 'eval_steps_per_second': 33.589, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 96, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 132352, with commas 132,352\n",
      "NUMBER OF TOKENS: 1024000, with commas 1,024,000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.526600</td>\n",
       "      <td>8.492445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 813170688000\n",
      "Human Readable: 813,170,688,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.3527250289917, 'eval_runtime': 3.4871, 'eval_samples_per_second': 68.252, 'eval_steps_per_second': 34.126, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 206400, with commas 206,400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1423360, with commas 1,423,360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='695' max='695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [695/695 00:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.012000</td>\n",
       "      <td>7.334210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1762689024000\n",
      "Human Readable: 1,762,689,024,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.193444728851318, 'eval_runtime': 3.7914, 'eval_samples_per_second': 62.774, 'eval_steps_per_second': 31.387, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1186176, with commas 1,186,176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 5188608, with commas 5,188,608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2534' max='2534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2534/2534 10:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>14.231200</td>\n",
       "      <td>6.937136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.190400</td>\n",
       "      <td>5.915351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.678500</td>\n",
       "      <td>5.660433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.502000</td>\n",
       "      <td>5.531826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.436800</td>\n",
       "      <td>5.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 36927613698048\n",
      "Human Readable: 36,927,613,698,048\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.423725605010986, 'eval_runtime': 12.5413, 'eval_samples_per_second': 18.977, 'eval_steps_per_second': 9.489, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 224, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 256, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 320, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 448, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
