{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from leap import LeapForCausalLM, LeapConfig\n",
    "from lstm import LstmForCausalLM\n",
    "from transformers import (PreTrainedTokenizerFast, TrainingArguments,\n",
    "                          Trainer, default_data_collator,\n",
    "                          GPT2Config, GPT2LMHeadModel)\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# word level tokenizer as per wikitext modeling\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf164de8b8cb4644812d93481e70e6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# globals\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "total_train_tokens = 105268829 # see appendix at the end of notebook\n",
    "max_num_params = 69308416\n",
    "param_data_ratio = max_num_params**.74 / total_train_tokens\n",
    "seq_len = 1024\n",
    "subset_datasets = raw_datasets\n",
    "\n",
    "# hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 5e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    max_grad_norm = 1,\n",
    "    fp16 = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-a2031eb206d20f87.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8f3dd2e5d5819fe2.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-f4668efef485cbea.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-9004e9678f0a1614.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-d189dc21c09aa046.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-50bce12b611e856c.arrow\n"
     ]
    }
   ],
   "source": [
    "# make a word level tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(pad_id = 0, pad_token = \"<pad>\")\n",
    "# no post processing\n",
    "\n",
    "# only use vocab size of 8192 for reasonable speed/memory\n",
    "token_trainer = WordLevelTrainer(vocab_size = 8191, # -1 for pad token\n",
    "                                 special_tokens = [\"<unk>\"])\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    text = raw_datasets[\"train\"]['text']\n",
    "    for i in range(0, len(text), batch_size):\n",
    "        yield text[i : i + batch_size]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(),\n",
    "                              trainer = token_trainer,\n",
    "                              length = len(raw_datasets[\"train\"][\"text\"]))\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer, pad_token = \"<pad>\")\n",
    "\n",
    "# tokenized the dataset\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[\"text\"])\n",
    "    return output\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched = True,\n",
    "    remove_columns = \"text\",\n",
    "    desc = f\"tokenize dataset\",\n",
    "    load_from_cache_file = True\n",
    ")\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + seq_len] for i in range(0, total_length, seq_len)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # for language modeling, inputs are labels (they will be shifted inside the model)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    # pad last block with 0\n",
    "    last_ids = result[\"input_ids\"][-1]\n",
    "    diff = seq_len - len(last_ids)\n",
    "    result[\"input_ids\"][-1] = last_ids + [0 for _ in range(diff)]\n",
    "    \n",
    "    # set attention mask to mask out these tokens\n",
    "    result[\"attention_mask\"][-1] = result[\"attention_mask\"][-1] + [0 for _ in range(diff)]\n",
    "    \n",
    "    # set pad labels to -100 so they will be ignored by CrossEntropyLoss\n",
    "    result[\"labels\"][-1] = result[\"labels\"][-1] + [-100 for _ in range(diff)]\n",
    "    return result\n",
    "\n",
    "# set globally block size for group texts function\n",
    "lm_dataset = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched = True,\n",
    "    batch_size = 10000,\n",
    "    desc = f\"Grouping texts in chunks of {seq_len}\",\n",
    "    load_from_cache_file = True\n",
    ")\n",
    "\n",
    "lm_dataset = lm_dataset.remove_columns([\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data(dataset, num_parameters, param_data_ratio):\n",
    "    dataset = DatasetDict(copy.deepcopy(dataset))\n",
    "    subset_num_tokens = num_parameters**.74 / param_data_ratio\n",
    "    \n",
    "    global seq_len\n",
    "    num_rows = int(subset_num_tokens) // seq_len\n",
    "\n",
    "    training_set = dataset[\"train\"]\n",
    "    dataset[\"train\"] = Dataset.from_dict(training_set[:num_rows+1])\n",
    "    \n",
    "    real_num_tokens = len(dataset[\"train\"]) * seq_len\n",
    "    print(f'NUMBER OF TOKENS: {real_num_tokens}, with commas {real_num_tokens:,}')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(hidden_size, n_head = None, gpt = False, rnn = False):\n",
    "    # calculate number of layers needed based on levine 2020\n",
    "    n_layer = round((math.log(hidden_size) - 5.039) / 5.55e-2)\n",
    "    n_layer = max(1, n_layer)\n",
    "    print(f'Using {n_layer} layers')\n",
    "    \n",
    "    # get number of parameters\n",
    "    if gpt is True:\n",
    "        config = GPT2Config(\n",
    "            n_embd = hidden_size, n_layer = n_layer,\n",
    "            n_head = 1, vocab_size = 0, n_positions = 0\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    elif rnn is True:\n",
    "        model = LstmForCausalLM(\n",
    "            hidden_size = hidden_size,\n",
    "            n_layer = n_layer,\n",
    "            vocab_size = 0\n",
    "        )\n",
    "    else:\n",
    "        config = LeapConfig(\n",
    "            hidden_size = hidden_size, n_layer = n_layer,\n",
    "            n_head = n_head, vocab_size = 0, n_positions = 0\n",
    "        )\n",
    "        model = LeapForCausalLM(config)\n",
    "\n",
    "    non_embedding_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'NON EMBEDDING PARAMETERS: {non_embedding_parameters}, with commas {non_embedding_parameters:,}')\n",
    "\n",
    "    # subset dataset using global lm_dataset\n",
    "    global lm_dataset\n",
    "    subset_datasets = subset_data(lm_dataset, non_embedding_parameters, param_data_ratio)\n",
    "\n",
    "    if gpt is True:\n",
    "        config = GPT2Config(\n",
    "            n_embd = hidden_size, n_layer = n_layer, n_head = n_head,\n",
    "            vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n",
    "            initializer_range = 1 / hidden_size**.5,\n",
    "            resid_pdrop = 0, embd_pdrop = 0, attn_pdrop = 0 # no dropout bc one epoch\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    elif rnn is True:\n",
    "        model = LstmForCausalLM(\n",
    "            hidden_size = hidden_size,\n",
    "            n_layer = n_layer,\n",
    "            vocab_size = len(tokenizer) + 1,\n",
    "            hidden_dropout_prob = 0\n",
    "        )\n",
    "    else:\n",
    "        config = LeapConfig(\n",
    "            hidden_size = hidden_size, n_layer = n_layer, n_head = n_head,\n",
    "            vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n",
    "            use_local_att = True, window_sizes = None, rescale = 10,\n",
    "            initializer_range = 1 / hidden_size**.5, hidden_dropout_prob = 0 # no dropout bc one epoch\n",
    "        )\n",
    "        model = LeapForCausalLM(config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator,\n",
    "        train_dataset=subset_datasets[\"train\"],\n",
    "        eval_dataset=subset_datasets[\"validation\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\\n\")\n",
    "    print(f'Numeric form: {int(trainer.state.total_flos)}\\nHuman Readable: {int(trainer.state.total_flos):,}')\n",
    "\n",
    "    print(\"\\n===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\\n\")\n",
    "    print(trainer.evaluate(subset_datasets[\"test\"]))\n",
    "\n",
    "    # save gpu memory\n",
    "    del trainer\n",
    "    del model\n",
    "    del subset_datasets\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 TRAINING\n",
    "Each run is done seperately in it's own cell just for easy viewing of logs and in case something goes wrong (OOM errors or training issues). Note that the learning rate had to be lowered from 1e-3 to 5e-4 because gpt2 wasn't converging on the largest test and all tests redone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 50112, with commas 50,112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 499712, with commas 499,712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 150249406464\n",
      "Human Readable: 150,249,406,464\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.038037300109863, 'eval_runtime': 0.7756, 'eval_samples_per_second': 306.864, 'eval_steps_per_second': 153.432, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 198528, with commas 198,528\n",
      "NUMBER OF TOKENS: 1382400, with commas 1,382,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.923600</td>\n",
       "      <td>5.628588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1646670643200\n",
      "Human Readable: 1,646,670,643,200\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.565395355224609, 'eval_runtime': 0.7777, 'eval_samples_per_second': 306.029, 'eval_steps_per_second': 153.015, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, n_head = 2, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 309600, with commas 309,600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1921024, with commas 1,921,024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.871900</td>\n",
       "      <td>5.523679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3568494182400\n",
      "Human Readable: 3,568,494,182,400\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.4598236083984375, 'eval_runtime': 0.7884, 'eval_samples_per_second': 301.859, 'eval_steps_per_second': 150.93, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, n_head = 2, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1779840, with commas 1,779,840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 7005184, with commas 7,005,184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3421' max='3421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3421/3421 02:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.880200</td>\n",
       "      <td>5.375783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.169800</td>\n",
       "      <td>5.121512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.002600</td>\n",
       "      <td>4.990281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.882700</td>\n",
       "      <td>4.893657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.810200</td>\n",
       "      <td>4.835835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.766800</td>\n",
       "      <td>4.809031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74808640143360\n",
      "Human Readable: 74,808,640,143,360\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.734671592712402, 'eval_runtime': 1.4655, 'eval_samples_per_second': 162.4, 'eval_steps_per_second': 81.2, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, n_head = 3, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 layers\n",
      "NON EMBEDDING PARAMETERS: 4235616, with commas 4,235,616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 13305856, with commas 13,305,856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6497' max='6497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6497/6497 08:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.934400</td>\n",
       "      <td>5.370236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.153100</td>\n",
       "      <td>5.088532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.973100</td>\n",
       "      <td>4.937986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.867700</td>\n",
       "      <td>4.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.770800</td>\n",
       "      <td>4.753085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.680800</td>\n",
       "      <td>4.669009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.598300</td>\n",
       "      <td>4.597792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.553300</td>\n",
       "      <td>4.530530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.474900</td>\n",
       "      <td>4.478220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.430700</td>\n",
       "      <td>4.430018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.386800</td>\n",
       "      <td>4.401743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.373100</td>\n",
       "      <td>4.389958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 338150979403776\n",
      "Human Readable: 338,150,979,403,776\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.317898750305176, 'eval_runtime': 2.5658, 'eval_samples_per_second': 92.757, 'eval_steps_per_second': 46.379, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 224, n_head = 4, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n",
      "NON EMBEDDING PARAMETERS: 7108352, with commas 7,108,352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 19518464, with commas 19,518,464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9531' max='9531' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9531/9531 16:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.947600</td>\n",
       "      <td>5.404133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.195800</td>\n",
       "      <td>5.097146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.990100</td>\n",
       "      <td>4.945464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.864600</td>\n",
       "      <td>4.837646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.778900</td>\n",
       "      <td>4.752940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.674500</td>\n",
       "      <td>4.662962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.593700</td>\n",
       "      <td>4.575736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.516000</td>\n",
       "      <td>4.498229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.429400</td>\n",
       "      <td>4.408222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.371200</td>\n",
       "      <td>4.340597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.285400</td>\n",
       "      <td>4.268012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.234500</td>\n",
       "      <td>4.210172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.177500</td>\n",
       "      <td>4.160933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.130300</td>\n",
       "      <td>4.124665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.096200</td>\n",
       "      <td>4.097829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.072500</td>\n",
       "      <td>4.076357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.048800</td>\n",
       "      <td>4.063325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.041900</td>\n",
       "      <td>4.057476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.038200</td>\n",
       "      <td>4.056235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 832464675667968\n",
      "Human Readable: 832,464,675,667,968\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.996950387954712, 'eval_runtime': 3.2961, 'eval_samples_per_second': 72.207, 'eval_steps_per_second': 36.103, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 256, n_head = 4, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 16029120, with commas 16,029,120\n",
      "NUMBER OF TOKENS: 35624960, with commas 35,624,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17395' max='17395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17395/17395 47:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.984400</td>\n",
       "      <td>5.439550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.209500</td>\n",
       "      <td>5.141995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.006600</td>\n",
       "      <td>4.971425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.885900</td>\n",
       "      <td>4.854750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.795700</td>\n",
       "      <td>4.794035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.707500</td>\n",
       "      <td>4.697477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.610100</td>\n",
       "      <td>4.607020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.541200</td>\n",
       "      <td>4.522990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.468200</td>\n",
       "      <td>4.428399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.380500</td>\n",
       "      <td>4.349230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.298400</td>\n",
       "      <td>4.272038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.217200</td>\n",
       "      <td>4.199662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>4.131761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.102500</td>\n",
       "      <td>4.064461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.055600</td>\n",
       "      <td>4.037722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.000800</td>\n",
       "      <td>3.973632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.942400</td>\n",
       "      <td>3.937718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.922100</td>\n",
       "      <td>3.903038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.886800</td>\n",
       "      <td>3.869648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.849700</td>\n",
       "      <td>3.840024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.822400</td>\n",
       "      <td>3.817307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.803300</td>\n",
       "      <td>3.784608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.764600</td>\n",
       "      <td>3.763017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.737400</td>\n",
       "      <td>3.743133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.744300</td>\n",
       "      <td>3.722451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.727500</td>\n",
       "      <td>3.707827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.700900</td>\n",
       "      <td>3.692480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.708200</td>\n",
       "      <td>3.680235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.688100</td>\n",
       "      <td>3.671085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.672000</td>\n",
       "      <td>3.661979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.666500</td>\n",
       "      <td>3.656528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.666600</td>\n",
       "      <td>3.652829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.657500</td>\n",
       "      <td>3.650912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.651300</td>\n",
       "      <td>3.649998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3426220553011200\n",
      "Human Readable: 3,426,220,553,011,200\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.617107391357422, 'eval_runtime': 5.4826, 'eval_samples_per_second': 43.41, 'eval_steps_per_second': 21.705, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 320, n_head = 5, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 layers\n",
      "NON EMBEDDING PARAMETERS: 45872064, with commas 45,872,064\n",
      "NUMBER OF TOKENS: 77564928, with commas 77,564,928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37874' max='37874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37874/37874 3:13:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.031200</td>\n",
       "      <td>5.466364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.266000</td>\n",
       "      <td>5.190466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.057200</td>\n",
       "      <td>5.023293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.956300</td>\n",
       "      <td>4.945972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.876700</td>\n",
       "      <td>4.826606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.765500</td>\n",
       "      <td>4.740273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.697000</td>\n",
       "      <td>4.677341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.633700</td>\n",
       "      <td>4.622077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.576200</td>\n",
       "      <td>4.559217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.496700</td>\n",
       "      <td>4.482879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.422400</td>\n",
       "      <td>4.392384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.337600</td>\n",
       "      <td>4.318099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.261900</td>\n",
       "      <td>4.227516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.190200</td>\n",
       "      <td>4.164449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.129900</td>\n",
       "      <td>4.111277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.064000</td>\n",
       "      <td>4.042594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.013800</td>\n",
       "      <td>4.008232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.979400</td>\n",
       "      <td>3.946411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.935200</td>\n",
       "      <td>3.904753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.874500</td>\n",
       "      <td>3.864056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.869000</td>\n",
       "      <td>3.833820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.817700</td>\n",
       "      <td>3.811317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.801900</td>\n",
       "      <td>3.776865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.765500</td>\n",
       "      <td>3.757130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.744700</td>\n",
       "      <td>3.723871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.713200</td>\n",
       "      <td>3.698801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.704400</td>\n",
       "      <td>3.671619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.666000</td>\n",
       "      <td>3.658937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.656400</td>\n",
       "      <td>3.630566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.642700</td>\n",
       "      <td>3.610764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>3.593423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.588100</td>\n",
       "      <td>3.575385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.582500</td>\n",
       "      <td>3.557119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.567900</td>\n",
       "      <td>3.548158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.560600</td>\n",
       "      <td>3.526461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.539900</td>\n",
       "      <td>3.509079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.509500</td>\n",
       "      <td>3.496561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.483500</td>\n",
       "      <td>3.476451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.512000</td>\n",
       "      <td>3.465210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.476600</td>\n",
       "      <td>3.456838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.465200</td>\n",
       "      <td>3.439033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.451200</td>\n",
       "      <td>3.428487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.420300</td>\n",
       "      <td>3.414819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.434800</td>\n",
       "      <td>3.405709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.432900</td>\n",
       "      <td>3.398905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.416900</td>\n",
       "      <td>3.386419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.399400</td>\n",
       "      <td>3.379016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.410700</td>\n",
       "      <td>3.365874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.390200</td>\n",
       "      <td>3.355592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.374300</td>\n",
       "      <td>3.346776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.350300</td>\n",
       "      <td>3.336448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.351100</td>\n",
       "      <td>3.330409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.344500</td>\n",
       "      <td>3.323724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.331900</td>\n",
       "      <td>3.315015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.339600</td>\n",
       "      <td>3.307005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.331500</td>\n",
       "      <td>3.298585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.307800</td>\n",
       "      <td>3.293266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.314400</td>\n",
       "      <td>3.288413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.315800</td>\n",
       "      <td>3.282645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.297300</td>\n",
       "      <td>3.277511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.313600</td>\n",
       "      <td>3.272749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.299000</td>\n",
       "      <td>3.268836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.295300</td>\n",
       "      <td>3.265192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.286600</td>\n",
       "      <td>3.260826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.287600</td>\n",
       "      <td>3.257252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.270400</td>\n",
       "      <td>3.254601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.269800</td>\n",
       "      <td>3.252039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.258300</td>\n",
       "      <td>3.250439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.281200</td>\n",
       "      <td>3.248230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.263900</td>\n",
       "      <td>3.247029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.277000</td>\n",
       "      <td>3.245785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.286500</td>\n",
       "      <td>3.245169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.263700</td>\n",
       "      <td>3.244645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.272700</td>\n",
       "      <td>3.244446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.272300</td>\n",
       "      <td>3.244391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 21348380048228352\n",
      "Human Readable: 21,348,380,048,228,352\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2382781505584717, 'eval_runtime': 10.8531, 'eval_samples_per_second': 21.929, 'eval_steps_per_second': 10.965, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 448, n_head = 7, gpt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEAP TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 49856, with commas 49,856\n",
      "NUMBER OF TOKENS: 497664, with commas 497,664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 148869218304\n",
      "Human Readable: 148,869,218,304\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.114893913269043, 'eval_runtime': 0.8957, 'eval_samples_per_second': 265.72, 'eval_steps_per_second': 132.86, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 198016, with commas 198,016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1380352, with commas 1,380,352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='674' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [674/674 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.107800</td>\n",
       "      <td>5.784122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1639990689792\n",
      "Human Readable: 1,639,990,689,792\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.721367835998535, 'eval_runtime': 0.8224, 'eval_samples_per_second': 289.388, 'eval_steps_per_second': 144.694, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, n_head = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 308960, with commas 308,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\desktop\\leap\\src\\leap\\LEAP.py:200: UserWarning: Using a hidden_size-to-head ratio of greater than 64 is not ideal as LEAP uses a simplified form of attention that relies on having many heads\n",
      "  warnings.warn(\"Using a hidden_size-to-head ratio of greater than 64 is not ideal as\"\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1917952, with commas 1,917,952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='937' max='937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [937/937 00:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.025500</td>\n",
       "      <td>5.665014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3555422699520\n",
      "Human Readable: 3,555,422,699,520\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.601402282714844, 'eval_runtime': 0.8223, 'eval_samples_per_second': 289.432, 'eval_steps_per_second': 144.716, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, n_head = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1776768, with commas 1,776,768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 6995968, with commas 6,995,968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3416' max='3416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3416/3416 02:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.088100</td>\n",
       "      <td>5.521639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.237200</td>\n",
       "      <td>5.147675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.968400</td>\n",
       "      <td>4.943083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.832500</td>\n",
       "      <td>4.823637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.733700</td>\n",
       "      <td>4.751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.679500</td>\n",
       "      <td>4.720037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74581272428544\n",
      "Human Readable: 74,581,272,428,544\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.645600318908691, 'eval_runtime': 1.5258, 'eval_samples_per_second': 155.988, 'eval_steps_per_second': 77.994, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, n_head = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 layers\n",
      "NON EMBEDDING PARAMETERS: 4229344, with commas 4,229,344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 13291520, with commas 13,291,520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6490' max='6490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6490/6490 07:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.155500</td>\n",
       "      <td>5.549298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.256500</td>\n",
       "      <td>5.126582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.980900</td>\n",
       "      <td>4.891197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.780600</td>\n",
       "      <td>4.744344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.655000</td>\n",
       "      <td>4.643302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.566100</td>\n",
       "      <td>4.545161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.485700</td>\n",
       "      <td>4.473008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.426700</td>\n",
       "      <td>4.422051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.375600</td>\n",
       "      <td>4.377839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.333000</td>\n",
       "      <td>4.350260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.311200</td>\n",
       "      <td>4.325484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.304700</td>\n",
       "      <td>4.317365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 337286462177280\n",
      "Human Readable: 337,286,462,177,280\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.252130031585693, 'eval_runtime': 2.175, 'eval_samples_per_second': 109.426, 'eval_steps_per_second': 54.713, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 224, n_head = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n",
      "NON EMBEDDING PARAMETERS: 7099136, with commas 7,099,136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 19499008, with commas 19,499,008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9521' max='9521' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9521/9521 12:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.206400</td>\n",
       "      <td>5.578196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.279000</td>\n",
       "      <td>5.132071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.963200</td>\n",
       "      <td>4.885261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.777000</td>\n",
       "      <td>4.734221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.647800</td>\n",
       "      <td>4.624836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.525700</td>\n",
       "      <td>4.528377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.448600</td>\n",
       "      <td>4.438150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.387900</td>\n",
       "      <td>4.363996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.321800</td>\n",
       "      <td>4.311187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.262100</td>\n",
       "      <td>4.262807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.208100</td>\n",
       "      <td>4.213320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.181500</td>\n",
       "      <td>4.173914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.152800</td>\n",
       "      <td>4.142035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.116600</td>\n",
       "      <td>4.116820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.085100</td>\n",
       "      <td>4.090932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.062700</td>\n",
       "      <td>4.077952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.069900</td>\n",
       "      <td>4.070109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.049700</td>\n",
       "      <td>4.066248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.054400</td>\n",
       "      <td>4.065651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 830556657942528\n",
      "Human Readable: 830,556,657,942,528\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.007962703704834, 'eval_runtime': 2.5397, 'eval_samples_per_second': 93.712, 'eval_steps_per_second': 46.856, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 256, n_head = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n",
      "NON EMBEDDING PARAMETERS: 16012480, with commas 16,012,480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 35598336, with commas 35,598,336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17382' max='17382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17382/17382 31:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.234500</td>\n",
       "      <td>5.631526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.340800</td>\n",
       "      <td>5.209981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.026300</td>\n",
       "      <td>4.926128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.796500</td>\n",
       "      <td>4.755405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.654400</td>\n",
       "      <td>4.624541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.545600</td>\n",
       "      <td>4.527535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.442900</td>\n",
       "      <td>4.407835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.361500</td>\n",
       "      <td>4.329208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.290500</td>\n",
       "      <td>4.259641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.221200</td>\n",
       "      <td>4.212588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.160500</td>\n",
       "      <td>4.154064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.121700</td>\n",
       "      <td>4.108780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.068200</td>\n",
       "      <td>4.061160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.035200</td>\n",
       "      <td>4.023276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.001700</td>\n",
       "      <td>3.987423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.963800</td>\n",
       "      <td>3.963372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.942000</td>\n",
       "      <td>3.930539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.913100</td>\n",
       "      <td>3.905960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.890100</td>\n",
       "      <td>3.882147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.875300</td>\n",
       "      <td>3.854211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.841900</td>\n",
       "      <td>3.836088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.830000</td>\n",
       "      <td>3.816982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.798400</td>\n",
       "      <td>3.799255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.781500</td>\n",
       "      <td>3.785859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.789600</td>\n",
       "      <td>3.772680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.772100</td>\n",
       "      <td>3.760230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.775800</td>\n",
       "      <td>3.747664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.755300</td>\n",
       "      <td>3.737905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.737700</td>\n",
       "      <td>3.730442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.716800</td>\n",
       "      <td>3.725642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.746700</td>\n",
       "      <td>3.720465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.707400</td>\n",
       "      <td>3.717974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.721600</td>\n",
       "      <td>3.716665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.710700</td>\n",
       "      <td>3.716166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3420105859399680\n",
      "Human Readable: 3,420,105,859,399,680\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.68304181098938, 'eval_runtime': 3.6836, 'eval_samples_per_second': 64.61, 'eval_steps_per_second': 32.305, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 320, n_head = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 45838016, with commas 45,838,016\n",
      "NUMBER OF TOKENS: 77522944, with commas 77,522,944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37853' max='37853' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37853/37853 2:03:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.327500</td>\n",
       "      <td>5.716834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.448100</td>\n",
       "      <td>5.326819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.160700</td>\n",
       "      <td>5.080652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.992200</td>\n",
       "      <td>4.923663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.814300</td>\n",
       "      <td>4.772987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.679600</td>\n",
       "      <td>4.605647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.544300</td>\n",
       "      <td>4.492287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.442400</td>\n",
       "      <td>4.409285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.360400</td>\n",
       "      <td>4.323043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.276600</td>\n",
       "      <td>4.250167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.202500</td>\n",
       "      <td>4.186943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.161100</td>\n",
       "      <td>4.127966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.112100</td>\n",
       "      <td>4.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.064800</td>\n",
       "      <td>4.033954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.011700</td>\n",
       "      <td>3.988789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.000500</td>\n",
       "      <td>3.959417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.963400</td>\n",
       "      <td>3.939453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.928900</td>\n",
       "      <td>3.900642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.891500</td>\n",
       "      <td>3.870284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.870300</td>\n",
       "      <td>3.853962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.834800</td>\n",
       "      <td>3.812873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.805300</td>\n",
       "      <td>3.805169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.797700</td>\n",
       "      <td>3.770089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.774600</td>\n",
       "      <td>3.750930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.749800</td>\n",
       "      <td>3.744701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.724700</td>\n",
       "      <td>3.722931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.727600</td>\n",
       "      <td>3.702272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>3.680955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.693600</td>\n",
       "      <td>3.672913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.657400</td>\n",
       "      <td>3.646818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.654100</td>\n",
       "      <td>3.638345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.633000</td>\n",
       "      <td>3.622716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>3.609097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.613500</td>\n",
       "      <td>3.589399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.598600</td>\n",
       "      <td>3.585317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.604100</td>\n",
       "      <td>3.568221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.583200</td>\n",
       "      <td>3.557877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.556500</td>\n",
       "      <td>3.547138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.556200</td>\n",
       "      <td>3.534039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.541500</td>\n",
       "      <td>3.525890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.550500</td>\n",
       "      <td>3.511067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.517500</td>\n",
       "      <td>3.509012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.524200</td>\n",
       "      <td>3.496013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.516400</td>\n",
       "      <td>3.485514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.502400</td>\n",
       "      <td>3.470456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.467600</td>\n",
       "      <td>3.463616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.475800</td>\n",
       "      <td>3.452965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.469900</td>\n",
       "      <td>3.446196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.453600</td>\n",
       "      <td>3.436713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.466000</td>\n",
       "      <td>3.428903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.450600</td>\n",
       "      <td>3.421304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.439700</td>\n",
       "      <td>3.415229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.440200</td>\n",
       "      <td>3.412559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.426600</td>\n",
       "      <td>3.405193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.414200</td>\n",
       "      <td>3.397887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.412900</td>\n",
       "      <td>3.391703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.413900</td>\n",
       "      <td>3.387043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.386400</td>\n",
       "      <td>3.381184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.404300</td>\n",
       "      <td>3.375121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.388500</td>\n",
       "      <td>3.369981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.397700</td>\n",
       "      <td>3.366617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.392800</td>\n",
       "      <td>3.363693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.403300</td>\n",
       "      <td>3.359738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.386100</td>\n",
       "      <td>3.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.374200</td>\n",
       "      <td>3.352945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.370900</td>\n",
       "      <td>3.349807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.367100</td>\n",
       "      <td>3.348014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>3.346146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.368000</td>\n",
       "      <td>3.344545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.358700</td>\n",
       "      <td>3.343566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.356800</td>\n",
       "      <td>3.342868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.357500</td>\n",
       "      <td>3.342073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.352400</td>\n",
       "      <td>3.341641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.363200</td>\n",
       "      <td>3.341480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.372000</td>\n",
       "      <td>3.341429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 21320987684634624\n",
      "Human Readable: 21,320,987,684,634,624\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.329268217086792, 'eval_runtime': 6.8597, 'eval_samples_per_second': 34.695, 'eval_steps_per_second': 17.348, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 448, n_head = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 22 layers\n",
      "NON EMBEDDING PARAMETERS: 69308416, with commas 69,308,416\n",
      "NUMBER OF TOKENS: 102674432, with commas 102,674,432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50134' max='50134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50134/50134 3:31:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.327900</td>\n",
       "      <td>5.709722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.474200</td>\n",
       "      <td>5.348254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.168500</td>\n",
       "      <td>5.094281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.998900</td>\n",
       "      <td>5.005149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.891400</td>\n",
       "      <td>4.878734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.770500</td>\n",
       "      <td>4.716152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.637200</td>\n",
       "      <td>4.609263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.519500</td>\n",
       "      <td>4.470326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.426500</td>\n",
       "      <td>4.394495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.345100</td>\n",
       "      <td>4.320373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.271700</td>\n",
       "      <td>4.238564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.193700</td>\n",
       "      <td>4.159550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.141500</td>\n",
       "      <td>4.104445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.073200</td>\n",
       "      <td>4.075042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.024800</td>\n",
       "      <td>4.019669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.012200</td>\n",
       "      <td>3.996829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.966200</td>\n",
       "      <td>3.946470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.917700</td>\n",
       "      <td>3.901366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.900100</td>\n",
       "      <td>3.886983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.900300</td>\n",
       "      <td>3.851015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.838700</td>\n",
       "      <td>3.836498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.830300</td>\n",
       "      <td>3.805321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.815700</td>\n",
       "      <td>3.785225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.775500</td>\n",
       "      <td>3.764884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.771400</td>\n",
       "      <td>3.746971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.743700</td>\n",
       "      <td>3.725244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.734100</td>\n",
       "      <td>3.710228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.692600</td>\n",
       "      <td>3.684728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>3.682437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.665000</td>\n",
       "      <td>3.660979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.645900</td>\n",
       "      <td>3.637091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.636200</td>\n",
       "      <td>3.626367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.619100</td>\n",
       "      <td>3.610645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.612000</td>\n",
       "      <td>3.602980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.632600</td>\n",
       "      <td>3.591956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>3.574160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.593900</td>\n",
       "      <td>3.569230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.581800</td>\n",
       "      <td>3.559101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.561600</td>\n",
       "      <td>3.560237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.565600</td>\n",
       "      <td>3.538358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.555800</td>\n",
       "      <td>3.524550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.535000</td>\n",
       "      <td>3.511157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.538000</td>\n",
       "      <td>3.501019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.523200</td>\n",
       "      <td>3.488410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.496000</td>\n",
       "      <td>3.480106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.494300</td>\n",
       "      <td>3.468717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.468100</td>\n",
       "      <td>3.460431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.468900</td>\n",
       "      <td>3.451229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.477200</td>\n",
       "      <td>3.444416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.468000</td>\n",
       "      <td>3.434717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.437300</td>\n",
       "      <td>3.424021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.449600</td>\n",
       "      <td>3.414134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.428500</td>\n",
       "      <td>3.415330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.426700</td>\n",
       "      <td>3.400129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.417600</td>\n",
       "      <td>3.398633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.420200</td>\n",
       "      <td>3.393151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.416000</td>\n",
       "      <td>3.382264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.399900</td>\n",
       "      <td>3.374866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.376700</td>\n",
       "      <td>3.366565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.394500</td>\n",
       "      <td>3.361937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.364700</td>\n",
       "      <td>3.356812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.388500</td>\n",
       "      <td>3.347431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.372500</td>\n",
       "      <td>3.342456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.360900</td>\n",
       "      <td>3.335491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.352800</td>\n",
       "      <td>3.328044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.349700</td>\n",
       "      <td>3.326084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.346800</td>\n",
       "      <td>3.318103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.337900</td>\n",
       "      <td>3.309631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.336800</td>\n",
       "      <td>3.303457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.327200</td>\n",
       "      <td>3.296417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.324600</td>\n",
       "      <td>3.292793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.313800</td>\n",
       "      <td>3.288776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.309900</td>\n",
       "      <td>3.283687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.309500</td>\n",
       "      <td>3.281479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.308600</td>\n",
       "      <td>3.275953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.299100</td>\n",
       "      <td>3.273052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.300100</td>\n",
       "      <td>3.269046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.288000</td>\n",
       "      <td>3.263762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.301200</td>\n",
       "      <td>3.260211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.297300</td>\n",
       "      <td>3.259489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.270400</td>\n",
       "      <td>3.256735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.275300</td>\n",
       "      <td>3.252541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.288900</td>\n",
       "      <td>3.249302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.270500</td>\n",
       "      <td>3.247640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.287200</td>\n",
       "      <td>3.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.286500</td>\n",
       "      <td>3.243650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.269200</td>\n",
       "      <td>3.241821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.248800</td>\n",
       "      <td>3.239884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.271500</td>\n",
       "      <td>3.238567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.260100</td>\n",
       "      <td>3.237079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.265000</td>\n",
       "      <td>3.235532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.279200</td>\n",
       "      <td>3.235108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.252100</td>\n",
       "      <td>3.234111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.254300</td>\n",
       "      <td>3.233676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.262100</td>\n",
       "      <td>3.232989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.271100</td>\n",
       "      <td>3.232745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.266900</td>\n",
       "      <td>3.232452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.255200</td>\n",
       "      <td>3.232282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.247400</td>\n",
       "      <td>3.232229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.279900</td>\n",
       "      <td>3.232212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 42697213473718272\n",
      "Human Readable: 42,697,213,473,718,272\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2232205867767334, 'eval_runtime': 8.8866, 'eval_samples_per_second': 26.782, 'eval_steps_per_second': 13.391, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 512, n_head = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "gpt2_param = np.array([\n",
    "    50112,\n",
    "    198528,\n",
    "    309600,\n",
    "    1779840,\n",
    "    4235616,\n",
    "    7108352,\n",
    "    16029120,\n",
    "    45872064,\n",
    "], dtype = 'int64')\n",
    "\n",
    "gpt2_y = np.array([\n",
    "    6.038,\n",
    "    5.565,\n",
    "    5.46,\n",
    "    4.735,\n",
    "    4.318,\n",
    "    3.997,\n",
    "    3.61,\n",
    "    3.23,\n",
    "])\n",
    "\n",
    "leap_param = np.array([\n",
    "    49856,\n",
    "    198016,\n",
    "    308960,\n",
    "    1776768,\n",
    "    4229344,\n",
    "    7099136,\n",
    "    16012480,\n",
    "    45838016,\n",
    "    69308416,\n",
    "], dtype = 'int64')\n",
    "\n",
    "gpt2_flos = np.array([\n",
    "    150249406464,\n",
    "    1646670643200,\n",
    "    3568494182400,\n",
    "    74808640143360,\n",
    "    338150979403776,\n",
    "    832464675667968,\n",
    "    3426220553011200,\n",
    "    21348380048228352,\n",
    "], dtype = 'int64')\n",
    "\n",
    "leap_flos = np.array([\n",
    "    148869218304,\n",
    "    1639990689792,\n",
    "    3555422699520,\n",
    "    74581272428544,\n",
    "    337286462177280,\n",
    "    830556657942528,\n",
    "    3420105859399680,\n",
    "    21320987684634624,\n",
    "    42697213473718272,\n",
    "], dtype = 'int64')\n",
    "\n",
    "\n",
    "leap_y = np.array([\n",
    "    6.115,\n",
    "    5.721,\n",
    "    5.601,\n",
    "    4.646,\n",
    "    4.252,\n",
    "    4.008,\n",
    "    3.683,\n",
    "    3.329,\n",
    "    3.223,\n",
    "])\n",
    "\n",
    "lstm_x = np.array([\n",
    "])\n",
    "\n",
    "lstm_y = np.array([\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-c2d83e27ad31>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n",
      "<ipython-input-16-c2d83e27ad31>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n",
      "<ipython-input-16-c2d83e27ad31>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n",
      "<ipython-input-16-c2d83e27ad31>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return (n_c / n)**a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 42932915937791.516 0.08903330047720337\n",
      "LEAP 28363515245975.594 0.09174780961091501\n",
      "GPT2 5.2206617991817826e+26 0.05117541902611786\n",
      "LEAP 2.4920167331379797e+26 0.05276556024262718\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def powerlaw(n, n_c, a):\n",
    "    return (n_c / n)**a\n",
    "\n",
    "\n",
    "# for parameters\n",
    "\n",
    "x = 10**np.arange(start = 4.3, stop = 8.5, step = .1, dtype = 'float64')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.loglog()\n",
    "plt.yticks([2, 3, 4, 5, 6, 7], labels = [2, 3, 4, 5, 6, 7])\n",
    "\n",
    "plt.scatter(x = gpt2_param, y = gpt2_y)\n",
    "(n_c, a), _ = curve_fit(powerlaw, gpt2_param, gpt2_y, maxfev=10000, p0 = np.array([9e13, .076]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'GPT2 powerlaw alpha = {-a:.3}', linestyle = '--')\n",
    "print(\"GPT2\", n_c, a)\n",
    "\n",
    "plt.scatter(x = leap_param, y = leap_y)\n",
    "(n_c, a), _ = curve_fit(powerlaw, leap_param, leap_y, maxfev=10000, p0 = np.array([9e13, .076]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'LEAP powerlaw alpha = {-a:.2}', linestyle = '--')\n",
    "print(\"LEAP\", n_c, a)\n",
    "\n",
    "plt.xlabel(\"Parameters\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# for comptute\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "x = 10**np.arange(start = 10.5, stop = 17.7, step = .1, dtype = 'float64')\n",
    "plt.loglog()\n",
    "plt.yticks([3, 4, 5, 6, 7], labels = [3, 4, 5, 6, 7])\n",
    "\n",
    "#plt.plot(gpt2_flos, gpt2_y, \".-\", color = 'C0', label = \"GPT2 linear interpolation\", markersize = 10)\n",
    "plt.scatter(x = gpt2_flos, y = gpt2_y)\n",
    "(n_c, a), _ = curve_fit(powerlaw, gpt2_flos, gpt2_y, maxfev=10000, p0 = np.array([9e30, .05]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'GPT2 powerlaw alpha = {-a:.3}', linestyle = '--')\n",
    "print(\"GPT2\", n_c, a)\n",
    "\n",
    "#plt.plot(leap_flos, leap_y, \".-\", color = 'C1', label = \"LEAP linear interpolation\", markersize = 10)\n",
    "plt.scatter(x = leap_flos, y = leap_y)\n",
    "(n_c, a), _ = curve_fit(powerlaw, leap_flos, leap_y, maxfev=10000, p0 = np.array([9e30, .05]))\n",
    "plt.plot(x, powerlaw(x, n_c, a), label = f'LEAP powerlaw alpha = {-a:.2}', linestyle = '--')\n",
    "print(\"LEAP\", n_c, a)\n",
    "\n",
    "plt.xlabel(\"FLOPs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('powerlaws.png', dpi = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105268829"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# to count tokens, comes from https://huggingface.co/docs/tokenizers/components\n",
    "whitespace_regex = re.compile(\"\\w+|[^\\w\\s]+\")\n",
    "\n",
    "# get number of tokens\n",
    "total_tokens = 0\n",
    "for row in raw_datasets[\"train\"][\"text\"]:\n",
    "    total_tokens += len((whitespace_regex.split(row)))\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 33408, with commas 33,408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 369664, with commas 369,664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='181' max='181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [181/181 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74098409472\n",
      "Human Readable: 74,098,409,472\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.131314277648926, 'eval_runtime': 3.2103, 'eval_samples_per_second': 74.137, 'eval_steps_per_second': 37.068, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 74688, with commas 74,688\n",
      "NUMBER OF TOKENS: 670720, with commas 670,720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='328' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [328/328 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 300568412160\n",
      "Human Readable: 300,568,412,160\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.494235038757324, 'eval_runtime': 3.5429, 'eval_samples_per_second': 67.177, 'eval_steps_per_second': 33.589, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 96, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 132352, with commas 132,352\n",
      "NUMBER OF TOKENS: 1024000, with commas 1,024,000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.526600</td>\n",
       "      <td>8.492445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 813170688000\n",
      "Human Readable: 813,170,688,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.3527250289917, 'eval_runtime': 3.4871, 'eval_samples_per_second': 68.252, 'eval_steps_per_second': 34.126, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 206400, with commas 206,400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1423360, with commas 1,423,360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='695' max='695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [695/695 00:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.012000</td>\n",
       "      <td>7.334210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1762689024000\n",
      "Human Readable: 1,762,689,024,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.193444728851318, 'eval_runtime': 3.7914, 'eval_samples_per_second': 62.774, 'eval_steps_per_second': 31.387, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1186176, with commas 1,186,176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 5188608, with commas 5,188,608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2534' max='2534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2534/2534 10:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>14.231200</td>\n",
       "      <td>6.937136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.190400</td>\n",
       "      <td>5.915351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.678500</td>\n",
       "      <td>5.660433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.502000</td>\n",
       "      <td>5.531826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.436800</td>\n",
       "      <td>5.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 36927613698048\n",
      "Human Readable: 36,927,613,698,048\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.423725605010986, 'eval_runtime': 12.5413, 'eval_samples_per_second': 18.977, 'eval_steps_per_second': 9.489, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 224, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 256, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 320, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 448, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
