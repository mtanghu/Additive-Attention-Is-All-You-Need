{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "from leap import LeapForCausalLM, LeapConfig\n", "from transformers import TrainingArguments, Trainer, default_data_collator\n", "from datasets import load_dataset\n", "\n", "from itertools import chain\n", "import logging\n", "logging.disable(logging.INFO)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# t5 tokenzier, warning is nothing to worry about since we will group the texts\n", "from transformers import AutoTokenizer\n", "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n", "column_names = raw_datasets[\"train\"].column_names\n", "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n", "block_size = 2048\n", "\n", "def tokenize_function(examples):\n", "    output = tokenizer(examples[text_column_name])\n", "    return output\n", "\n", "def group_texts(examples):\n", "    # concatenate text\n", "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n", "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n", "    \n", "    # drop last block\n", "    if total_length >= block_size:\n", "        total_length = (total_length // block_size) * block_size\n", "\n", "    # split by chunks of block_size\n", "    result = {\n", "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n", "        for k, t in concatenated_examples.items()\n", "    }\n", "    result[\"labels\"] = result[\"input_ids\"].copy()\n", "    return result\n", "\n", "tokenized_datasets = raw_datasets.map(\n", "            tokenize_function,\n", "            batched=True,\n", "            num_proc=1,\n", "            remove_columns=column_names,\n", "        )\n", "\n", "lm_dataset = tokenized_datasets.map(\n", "    group_texts,\n", "    batched=True,\n", "    num_proc=1,\n", "    desc=f\"Grouping texts in chunks of {block_size}\",\n", ")\n", "\n", "lm_dataset.set_format('pt')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# hyperparameters\n", "training_args = TrainingArguments(\n", "    output_dir = \"./results\",\n", "    logging_strategy = \"epoch\",\n", "    evaluation_strategy = \"epoch\",\n", "    save_strategy = \"epoch\",\n", "    report_to = \"none\",\n", "    learning_rate = 5e-4, \n", "    num_train_epochs = 10,\n", "    per_device_train_batch_size = 4,\n", "    per_device_eval_batch_size = 4,\n", "    load_best_model_at_end = True,\n", "    metric_for_best_model = \"eval_loss\",\n", "    max_grad_norm = 1,\n", "    fp16 = True\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_leap(window_sizes):\n", "    config = LeapConfig(\n", "        hidden_size = 128,\n", "        vocab_size = len(tokenizer),\n", "        n_positions = block_size,\n", "        n_heads = 4,\n", "        n_layer = len(window_sizes),\n", "        use_local_att = True,\n", "        window_sizes = window_sizes, # will be set automatically\n", "        hidden_dropout_prob = .1,\n", "        rescale = 10\n", "    )\n", "    print(config.window_sizes)\n", "    window_model = LeapForCausalLM(config)\n", "    \n", "    window_trainer = Trainer(\n", "        model=window_model,\n", "        args=training_args,\n", "        data_collator=default_data_collator,\n", "        train_dataset=lm_dataset[\"train\"],\n", "        eval_dataset=lm_dataset[\"validation\"]\n", "    )\n", "\n", "    window_trainer.train()\n", "    \n", "    # free gpu memory\n", "    del window_trainer\n", "    window_model.cpu()\n", "    torch.cuda.empty_cache()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Simple experiments to find a heuristic for window values\n", "\n", "Having the first and last layer be global attention is for sure. The first global attention layer should establish \"context\" like if there's task metadata at the start of the sequence. The last global attention layer to accumulate  all information. "], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Let's start with a baseline\n", "\n", "The first and last layers are global attention (as stated before) and of course we need local attention so let's try a window size of 4."], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 2048]\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 06:34, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.403400</td>\n", "      <td>5.614212</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.718500</td>\n", "      <td>5.347041</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.473900</td>\n", "      <td>5.140350</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.281300</td>\n", "      <td>5.001758</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.145200</td>\n", "      <td>4.916826</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.044400</td>\n", "      <td>4.843692</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.970400</td>\n", "      <td>4.792968</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.915800</td>\n", "      <td>4.760973</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.877600</td>\n", "      <td>4.742640</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.855600</td>\n", "      <td>4.734987</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Now try a local window size of 8 to compare"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 06:21, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.417200</td>\n", "      <td>5.626072</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.730600</td>\n", "      <td>5.369969</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.491900</td>\n", "      <td>5.174758</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.310600</td>\n", "      <td>5.040751</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.180400</td>\n", "      <td>4.953936</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.083100</td>\n", "      <td>4.892673</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>5.010500</td>\n", "      <td>4.835805</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.957700</td>\n", "      <td>4.805156</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.920500</td>\n", "      <td>4.786395</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.898000</td>\n", "      <td>4.778767</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 8, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Seems slightly worse, let's move on. Try adding a second local attention layer vs adding another global attention layer"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 4, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 06:59, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.408400</td>\n", "      <td>5.614400</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.715400</td>\n", "      <td>5.354980</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.476300</td>\n", "      <td>5.146724</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.277700</td>\n", "      <td>4.997527</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.132900</td>\n", "      <td>4.891938</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.029600</td>\n", "      <td>4.823465</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.951300</td>\n", "      <td>4.765365</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.894500</td>\n", "      <td>4.735488</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.855300</td>\n", "      <td>4.713768</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.831800</td>\n", "      <td>4.704624</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 4, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 2048, 2048]\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 06:54, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.397600</td>\n", "      <td>5.615664</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.720400</td>\n", "      <td>5.347426</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.468500</td>\n", "      <td>5.142619</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.278800</td>\n", "      <td>5.005457</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.145600</td>\n", "      <td>4.910377</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.047000</td>\n", "      <td>4.841533</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.972100</td>\n", "      <td>4.789617</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.916700</td>\n", "      <td>4.759144</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.877400</td>\n", "      <td>4.736964</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.854500</td>\n", "      <td>4.729196</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 2048, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. How about also trying 8"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 06:55, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.407400</td>\n", "      <td>5.610317</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.710700</td>\n", "      <td>5.349531</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.468800</td>\n", "      <td>5.135484</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.268200</td>\n", "      <td>4.987660</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.124800</td>\n", "      <td>4.884447</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.021300</td>\n", "      <td>4.814922</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.942300</td>\n", "      <td>4.753984</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.885200</td>\n", "      <td>4.722650</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.845500</td>\n", "      <td>4.700609</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.821600</td>\n", "      <td>4.691215</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Okay, the 4 and 8 local windows are the best so far. Let's try another local window of double the size of the previous local window comparing against just adding another 8 layer"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 07:29, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.413400</td>\n", "      <td>5.598911</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.693700</td>\n", "      <td>5.320351</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.438600</td>\n", "      <td>5.114291</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.245200</td>\n", "      <td>4.965509</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.103500</td>\n", "      <td>4.867014</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.999400</td>\n", "      <td>4.791622</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.920800</td>\n", "      <td>4.734686</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.863700</td>\n", "      <td>4.698231</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.822700</td>\n", "      <td>4.677152</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.800600</td>\n", "      <td>4.668426</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 07:24, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.415500</td>\n", "      <td>5.602804</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.698700</td>\n", "      <td>5.326443</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.445000</td>\n", "      <td>5.120928</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.251900</td>\n", "      <td>4.972217</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.110500</td>\n", "      <td>4.872621</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.006700</td>\n", "      <td>4.796437</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.927900</td>\n", "      <td>4.740082</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.870900</td>\n", "      <td>4.704421</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.830100</td>\n", "      <td>4.683311</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.808000</td>\n", "      <td>4.674782</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 8, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Adding 16 is best, let's continue the pattern of adding a local window of double the size. Let's compare against adding a 4 instead, and try also another global layer"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 32, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 07:54, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.368600</td>\n", "      <td>5.593536</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.676700</td>\n", "      <td>5.310952</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.425300</td>\n", "      <td>5.093761</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.230900</td>\n", "      <td>4.957909</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.091400</td>\n", "      <td>4.851718</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.986400</td>\n", "      <td>4.780315</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.907500</td>\n", "      <td>4.719919</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.849100</td>\n", "      <td>4.683742</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.809200</td>\n", "      <td>4.665123</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.784500</td>\n", "      <td>4.655414</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 32, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 4, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 08:00, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.370000</td>\n", "      <td>5.593909</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.677300</td>\n", "      <td>5.309531</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.423700</td>\n", "      <td>5.092208</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.228600</td>\n", "      <td>4.955739</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.089800</td>\n", "      <td>4.849909</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.985100</td>\n", "      <td>4.780025</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.906500</td>\n", "      <td>4.718997</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.847800</td>\n", "      <td>4.682447</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.807900</td>\n", "      <td>4.663561</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.782900</td>\n", "      <td>4.653922</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 4, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 08:07, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.370100</td>\n", "      <td>5.590558</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.675600</td>\n", "      <td>5.310473</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.423300</td>\n", "      <td>5.092281</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.228500</td>\n", "      <td>4.954886</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.089400</td>\n", "      <td>4.850015</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.984800</td>\n", "      <td>4.778233</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.906700</td>\n", "      <td>4.720803</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.849100</td>\n", "      <td>4.686124</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.809500</td>\n", "      <td>4.666592</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.784900</td>\n", "      <td>4.657202</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. They all seem to work well, though restarting back to 4 seems to work best. Let's continue with adding and 8 vs adding a global layer BEFORE the 4 (the idea being to alternate between global and local attention)"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 4, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 08:29, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.357700</td>\n", "      <td>5.587782</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.673200</td>\n", "      <td>5.309240</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.413900</td>\n", "      <td>5.093601</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.214500</td>\n", "      <td>4.946199</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.075600</td>\n", "      <td>4.841637</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.973500</td>\n", "      <td>4.767156</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.895100</td>\n", "      <td>4.711235</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.837000</td>\n", "      <td>4.674083</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.796600</td>\n", "      <td>4.652811</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.772200</td>\n", "      <td>4.644971</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 4, 8, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048, 4, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 08:28, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.356200</td>\n", "      <td>5.586043</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.671900</td>\n", "      <td>5.305367</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.408200</td>\n", "      <td>5.082562</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.207600</td>\n", "      <td>4.938675</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.068900</td>\n", "      <td>4.835489</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.966600</td>\n", "      <td>4.762750</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.888800</td>\n", "      <td>4.705896</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.830600</td>\n", "      <td>4.670035</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.790600</td>\n", "      <td>4.648480</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.766400</td>\n", "      <td>4.639331</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048, 4, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. So alternation might work! If that's true let's try getting rid of the first global attention layer (and using the alternating strategy), vs leaving it and not alternating"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[4, 8, 16, 2048, 4, 8, 16, 2048]\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:05, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.447600</td>\n", "      <td>5.701015</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.798900</td>\n", "      <td>5.437718</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.554800</td>\n", "      <td>5.219329</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.360300</td>\n", "      <td>5.061584</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.210400</td>\n", "      <td>4.942722</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>5.094500</td>\n", "      <td>4.855447</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>5.005600</td>\n", "      <td>4.793694</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.941300</td>\n", "      <td>4.749907</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.897200</td>\n", "      <td>4.725306</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.871700</td>\n", "      <td>4.716165</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([4, 8, 16, 2048, 4, 8, 16, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 4, 8, 16, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:11, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.379700</td>\n", "      <td>5.578846</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.665700</td>\n", "      <td>5.309939</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.409700</td>\n", "      <td>5.077781</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.203500</td>\n", "      <td>4.924696</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.057800</td>\n", "      <td>4.821679</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.950900</td>\n", "      <td>4.743570</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.870600</td>\n", "      <td>4.690719</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.812900</td>\n", "      <td>4.654828</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.771900</td>\n", "      <td>4.633359</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.747100</td>\n", "      <td>4.624532</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 4, 8, 16, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Okay so looks like there should be global attention up front (good to clarify that at least), let's back up and just try the alternating strategy with global attention up front (comparing non alternating strategy directly above)"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048, 4, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 08:58, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.379300</td>\n", "      <td>5.576530</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.666200</td>\n", "      <td>5.304617</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.407900</td>\n", "      <td>5.078589</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.200300</td>\n", "      <td>4.926067</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.053500</td>\n", "      <td>4.822440</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.946000</td>\n", "      <td>4.742070</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.865100</td>\n", "      <td>4.686777</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.806900</td>\n", "      <td>4.651878</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.765700</td>\n", "      <td>4.628971</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.740800</td>\n", "      <td>4.619902</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048, 4, 8, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. That seems to work well, let's compare that to maybe trying faster cycle rates (compared to baseline 4, 8, 16)"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 16, 2048, 4, 16, 2048, 4, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:27, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.337400</td>\n", "      <td>5.570338</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.654300</td>\n", "      <td>5.292899</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.396700</td>\n", "      <td>5.068539</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.196800</td>\n", "      <td>4.919131</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.054200</td>\n", "      <td>4.819403</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.949100</td>\n", "      <td>4.747835</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.869400</td>\n", "      <td>4.690602</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.810800</td>\n", "      <td>4.650536</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.768500</td>\n", "      <td>4.629301</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.744200</td>\n", "      <td>4.620091</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 16, 2048, 4, 16, 2048, 4, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 2048, 4, 8, 2048, 4, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:23, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.341100</td>\n", "      <td>5.579933</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.662300</td>\n", "      <td>5.301711</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.406600</td>\n", "      <td>5.078926</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.203200</td>\n", "      <td>4.926170</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.061500</td>\n", "      <td>4.829375</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.956000</td>\n", "      <td>4.757246</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.877600</td>\n", "      <td>4.698025</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.818400</td>\n", "      <td>4.658959</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.776700</td>\n", "      <td>4.636538</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.752400</td>\n", "      <td>4.627797</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 2048, 4, 8, 2048, 4, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 8, 16, 2048, 8, 16, 2048, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:29, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.341200</td>\n", "      <td>5.585158</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.672700</td>\n", "      <td>5.320930</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.424400</td>\n", "      <td>5.105385</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.231100</td>\n", "      <td>4.962701</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.094200</td>\n", "      <td>4.868821</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.993000</td>\n", "      <td>4.796904</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.917200</td>\n", "      <td>4.742193</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.860800</td>\n", "      <td>4.706789</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.821200</td>\n", "      <td>4.685561</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.797400</td>\n", "      <td>4.676823</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 8, 16, 2048, 8, 16, 2048, 8, 2048])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048, 4, 8, 16, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:25, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.337000</td>\n", "      <td>5.573453</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.651800</td>\n", "      <td>5.286858</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.393000</td>\n", "      <td>5.064430</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.192500</td>\n", "      <td>4.914081</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.049900</td>\n", "      <td>4.814898</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.944500</td>\n", "      <td>4.743355</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.864900</td>\n", "      <td>4.685818</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.805700</td>\n", "      <td>4.646133</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.764000</td>\n", "      <td>4.622876</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.739900</td>\n", "      <td>4.614115</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048, 4, 8, 16, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Looks like 4, 8, 16 is best. Though, what if the last layer wasn't global?"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048, 4, 8, 16]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:00, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.378000</td>\n", "      <td>5.577907</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.665000</td>\n", "      <td>5.303333</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.405800</td>\n", "      <td>5.074307</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.198600</td>\n", "      <td>4.922642</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.052600</td>\n", "      <td>4.819136</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.944600</td>\n", "      <td>4.740305</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.863700</td>\n", "      <td>4.685725</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.805600</td>\n", "      <td>4.650772</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.764200</td>\n", "      <td>4.627913</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.739400</td>\n", "      <td>4.618880</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048, 4, 8, 16])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[2048, 4, 8, 16, 2048, 4, 8, 2048]\n"]}, {"name": "stderr", "output_type": "stream", "text": ["C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n", "  warnings.warn(\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [3340/3340 09:05, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Epoch</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>6.379300</td>\n", "      <td>5.576530</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>5.666200</td>\n", "      <td>5.304617</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>5.407900</td>\n", "      <td>5.078589</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>5.200300</td>\n", "      <td>4.926067</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>5.053500</td>\n", "      <td>4.822440</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>4.946000</td>\n", "      <td>4.742070</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>4.865100</td>\n", "      <td>4.686777</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>4.806900</td>\n", "      <td>4.651878</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>4.765700</td>\n", "      <td>4.628971</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>4.740800</td>\n", "      <td>4.619902</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["run_leap([2048, 4, 8, 16, 2048, 4, 8, 2048])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Conclusion for now: first layer should be global attention, layers of sizes 4, then 8, then 16, then global attention, and repeat."], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}