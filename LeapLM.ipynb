{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from leap import LeapForCausalLM, LeapConfig\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "from datasets import load_dataset\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5 tokenzier, warning is nothing to worry about since we will group the texts\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "block_size = 2048\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[text_column_name])\n",
    "    return output\n",
    "\n",
    "def group_texts(examples):\n",
    "    # concatenate text\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # drop last block\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # split by chunks of block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "\n",
    "lm_dataset = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "lm_dataset.set_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 5e-4, \n",
    "    num_train_epochs = 10,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    max_grad_norm = 1,\n",
    "    fp16 = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LEAP Transformer (with windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LeapConfig(\n",
    "    hidden_size = 128,\n",
    "    vocab_size = len(tokenizer),\n",
    "    n_positions = block_size,\n",
    "    n_heads = 4,\n",
    "    n_layer = 6,\n",
    "    use_local_att = True,\n",
    "    window_sizes = None, # will be set automatically\n",
    "    hidden_dropout_prob = .1,\n",
    "    rescale = 10\n",
    ")\n",
    "window_model = LeapForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in window_model.parameters() if p.requires_grad)\n",
    "f'{(pytorch_total_params / 1e6):2.1f}M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_trainer = Trainer(\n",
    "    model=window_model,\n",
    "    args=training_args,\n",
    "    data_collator=default_data_collator,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "window_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free gpu memory\n",
    "del window_trainer\n",
    "window_model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code to try some text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test datapoint\n",
    "test_number = 0\n",
    "\n",
    "input_ids = lm_dataset[\"test\"][test_number][\"input_ids\"].reshape(1, -1).cpu()\n",
    "attention_mask = lm_dataset[\"test\"][test_number][\"attention_mask\"].reshape(1, -1).cpu()\n",
    "labels = lm_dataset[\"test\"][test_number][\"labels\"].reshape(1, -1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model on cpu\n",
    "test_model = window_model\n",
    "test_model = test_model.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prompt\n",
    "tokenizer.batch_decode(input_ids[:,:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the text\n",
    "generated_tokens = test_model.generate(input_ids[:,:50], attention_mask = attention_mask, do_sample=True, max_length=256, temperature = .7)\n",
    "tokenizer.batch_decode(generated_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
