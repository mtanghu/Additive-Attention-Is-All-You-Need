{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from leap import LeapForCausalLM, LeapConfig\n",
    "from lstm import LstmForCausalLM\n",
    "from transformers import (PreTrainedTokenizerFast, TrainingArguments,\n",
    "                          Trainer, default_data_collator,\n",
    "                          GPT2Config, GPT2LMHeadModel)\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# word level tokenizer as per wikitext modeling\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from itertools import chain\n",
    "import logging\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf164de8b8cb4644812d93481e70e6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# globals\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "total_train_tokens = 105268829 # see appendix at the end of notebook\n",
    "max_num_params = 69308416\n",
    "param_data_ratio = max_num_params**.74 / total_train_tokens\n",
    "seq_len = 1024\n",
    "subset_datasets = raw_datasets\n",
    "\n",
    "# hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 5e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    max_grad_norm = 1,\n",
    "    fp16 = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-a2031eb206d20f87.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8f3dd2e5d5819fe2.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-f4668efef485cbea.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-9004e9678f0a1614.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-d189dc21c09aa046.arrow\n",
      "Loading cached processed dataset at C:/Users/micha/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-50bce12b611e856c.arrow\n"
     ]
    }
   ],
   "source": [
    "# make a word level tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(pad_id = 0, pad_token = \"<pad>\")\n",
    "# no post processing\n",
    "\n",
    "# only use vocab size of 8192 for reasonable speed/memory\n",
    "token_trainer = WordLevelTrainer(vocab_size = 8191, # -1 for pad token\n",
    "                                 special_tokens = [\"<unk>\"])\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    text = raw_datasets[\"train\"]['text']\n",
    "    for i in range(0, len(text), batch_size):\n",
    "        yield text[i : i + batch_size]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(),\n",
    "                              trainer = token_trainer,\n",
    "                              length = len(raw_datasets[\"train\"][\"text\"]))\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer, pad_token = \"<pad>\")\n",
    "\n",
    "# tokenized the dataset\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[\"text\"])\n",
    "    return output\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched = True,\n",
    "    remove_columns = \"text\",\n",
    "    desc = f\"tokenize dataset\",\n",
    "    load_from_cache_file = True\n",
    ")\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + seq_len] for i in range(0, total_length, seq_len)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # for language modeling, inputs are labels (they will be shifted inside the model)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    # pad last block with 0\n",
    "    last_ids = result[\"input_ids\"][-1]\n",
    "    diff = seq_len - len(last_ids)\n",
    "    result[\"input_ids\"][-1] = last_ids + [0 for _ in range(diff)]\n",
    "    \n",
    "    # set attention mask to mask out these tokens\n",
    "    result[\"attention_mask\"][-1] = result[\"attention_mask\"][-1] + [0 for _ in range(diff)]\n",
    "    \n",
    "    # set pad labels to -100 so they will be ignored by CrossEntropyLoss\n",
    "    result[\"labels\"][-1] = result[\"labels\"][-1] + [-100 for _ in range(diff)]\n",
    "    return result\n",
    "\n",
    "# set globally block size for group texts function\n",
    "lm_dataset = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched = True,\n",
    "    batch_size = 10000,\n",
    "    desc = f\"Grouping texts in chunks of {seq_len}\",\n",
    "    load_from_cache_file = True\n",
    ")\n",
    "\n",
    "lm_dataset = lm_dataset.remove_columns([\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data(dataset, num_parameters, param_data_ratio):\n",
    "    dataset = DatasetDict(copy.deepcopy(dataset))\n",
    "    subset_num_tokens = num_parameters**.74 / param_data_ratio\n",
    "    \n",
    "    global seq_len\n",
    "    num_rows = int(subset_num_tokens) // seq_len\n",
    "\n",
    "    training_set = dataset[\"train\"]\n",
    "    dataset[\"train\"] = Dataset.from_dict(training_set[:num_rows+1])\n",
    "    \n",
    "    real_num_tokens = len(dataset[\"train\"]) * seq_len\n",
    "    print(f'NUMBER OF TOKENS: {real_num_tokens}, with commas {real_num_tokens:,}')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(hidden_size, n_head = None, gpt = False, rnn = False):\n",
    "    # calculate number of layers needed based on levine 2020\n",
    "    n_layer = round((math.log(hidden_size) - 5.039) / 5.55e-2)\n",
    "    n_layer = max(1, n_layer)\n",
    "    print(f'Using {n_layer} layers')\n",
    "    \n",
    "    # get number of parameters\n",
    "    if gpt is True:\n",
    "        config = GPT2Config(\n",
    "            n_embd = hidden_size, n_layer = n_layer,\n",
    "            n_head = 1, vocab_size = 0, n_positions = 0\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    elif rnn is True:\n",
    "        model = LstmForCausalLM(\n",
    "            hidden_size = hidden_size,\n",
    "            n_layer = n_layer,\n",
    "            vocab_size = 0\n",
    "        )\n",
    "    else:\n",
    "        config = LeapConfig(\n",
    "            hidden_size = hidden_size, n_layer = n_layer,\n",
    "            n_head = n_head, vocab_size = 0, n_positions = 0\n",
    "        )\n",
    "        model = LeapForCausalLM(config)\n",
    "\n",
    "    non_embedding_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'NON EMBEDDING PARAMETERS: {non_embedding_parameters}, with commas {non_embedding_parameters:,}')\n",
    "\n",
    "    # subset dataset using global lm_dataset\n",
    "    global lm_dataset\n",
    "    subset_datasets = subset_data(lm_dataset, non_embedding_parameters, param_data_ratio)\n",
    "\n",
    "    if gpt is True:\n",
    "        config = GPT2Config(\n",
    "            n_embd = hidden_size, n_layer = n_layer, n_head = n_head,\n",
    "            vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n",
    "            initializer_range = 1 / hidden_size**.5,\n",
    "            resid_pdrop = 0, embd_pdrop = 0, attn_pdrop = 0 # no dropout bc one epoch\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    elif rnn is True:\n",
    "        model = LstmForCausalLM(\n",
    "            hidden_size = hidden_size,\n",
    "            n_layer = n_layer,\n",
    "            vocab_size = len(tokenizer) + 1,\n",
    "            hidden_dropout_prob = 0\n",
    "        )\n",
    "    else:\n",
    "        config = LeapConfig(\n",
    "            hidden_size = hidden_size, n_layer = n_layer, n_head = n_head,\n",
    "            vocab_size = len(tokenizer) + 1, n_positions = seq_len,\n",
    "            use_local_att = True, window_sizes = None, rescale = 10,\n",
    "            initializer_range = 1 / hidden_size**.5, hidden_dropout_prob = 0 # no dropout bc one epoch\n",
    "        )\n",
    "        model = LeapForCausalLM(config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator,\n",
    "        train_dataset=subset_datasets[\"train\"],\n",
    "        eval_dataset=subset_datasets[\"validation\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\\n\")\n",
    "    print(f'Numeric form: {int(trainer.state.total_flos)}\\nHuman Readable: {int(trainer.state.total_flos):,}')\n",
    "\n",
    "    print(\"\\n===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\\n\")\n",
    "    print(trainer.evaluate(subset_datasets[\"test\"]))\n",
    "\n",
    "    # save gpu memory\n",
    "    del trainer\n",
    "    del model\n",
    "    del subset_datasets\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 TRAINING\n",
    "Each run is done seperately in it's own cell just for easy viewing of logs and in case something goes wrong (OOM errors or training issues). Note that the learning rate had to be lowered from 1e-3 to 5e-4 because gpt2 wasn't converging on the largest test and all tests redone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 50112, with commas 50,112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 499712, with commas 499,712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 150249406464\n",
      "Human Readable: 150,249,406,464\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.038037300109863, 'eval_runtime': 0.7756, 'eval_samples_per_second': 306.864, 'eval_steps_per_second': 153.432, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 198528, with commas 198,528\n",
      "NUMBER OF TOKENS: 1382400, with commas 1,382,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.923600</td>\n",
       "      <td>5.628588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1646670643200\n",
      "Human Readable: 1,646,670,643,200\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.565395355224609, 'eval_runtime': 0.7777, 'eval_samples_per_second': 306.029, 'eval_steps_per_second': 153.015, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, n_head = 2, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 309600, with commas 309,600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1921024, with commas 1,921,024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.871900</td>\n",
       "      <td>5.523679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3568494182400\n",
      "Human Readable: 3,568,494,182,400\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.4598236083984375, 'eval_runtime': 0.7884, 'eval_samples_per_second': 301.859, 'eval_steps_per_second': 150.93, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, n_head = 2, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1779840, with commas 1,779,840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 7005184, with commas 7,005,184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3421' max='3421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3421/3421 02:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.880200</td>\n",
       "      <td>5.375783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.169800</td>\n",
       "      <td>5.121512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.002600</td>\n",
       "      <td>4.990281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.882700</td>\n",
       "      <td>4.893657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.810200</td>\n",
       "      <td>4.835835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.766800</td>\n",
       "      <td>4.809031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74808640143360\n",
      "Human Readable: 74,808,640,143,360\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.734671592712402, 'eval_runtime': 1.4655, 'eval_samples_per_second': 162.4, 'eval_steps_per_second': 81.2, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, n_head = 3, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 layers\n",
      "NON EMBEDDING PARAMETERS: 4235616, with commas 4,235,616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 13305856, with commas 13,305,856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6497' max='6497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6497/6497 08:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.934400</td>\n",
       "      <td>5.370236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.153100</td>\n",
       "      <td>5.088532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.973100</td>\n",
       "      <td>4.937986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.867700</td>\n",
       "      <td>4.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.770800</td>\n",
       "      <td>4.753085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.680800</td>\n",
       "      <td>4.669009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.598300</td>\n",
       "      <td>4.597792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.553300</td>\n",
       "      <td>4.530530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.474900</td>\n",
       "      <td>4.478220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.430700</td>\n",
       "      <td>4.430018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.386800</td>\n",
       "      <td>4.401743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.373100</td>\n",
       "      <td>4.389958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 338150979403776\n",
      "Human Readable: 338,150,979,403,776\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.317898750305176, 'eval_runtime': 2.5658, 'eval_samples_per_second': 92.757, 'eval_steps_per_second': 46.379, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 224, n_head = 4, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n",
      "NON EMBEDDING PARAMETERS: 7108352, with commas 7,108,352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 19518464, with commas 19,518,464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9531' max='9531' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9531/9531 16:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.947600</td>\n",
       "      <td>5.404133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.195800</td>\n",
       "      <td>5.097146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.990100</td>\n",
       "      <td>4.945464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.864600</td>\n",
       "      <td>4.837646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.778900</td>\n",
       "      <td>4.752940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.674500</td>\n",
       "      <td>4.662962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.593700</td>\n",
       "      <td>4.575736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.516000</td>\n",
       "      <td>4.498229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.429400</td>\n",
       "      <td>4.408222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.371200</td>\n",
       "      <td>4.340597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.285400</td>\n",
       "      <td>4.268012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.234500</td>\n",
       "      <td>4.210172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.177500</td>\n",
       "      <td>4.160933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.130300</td>\n",
       "      <td>4.124665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.096200</td>\n",
       "      <td>4.097829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.072500</td>\n",
       "      <td>4.076357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.048800</td>\n",
       "      <td>4.063325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.041900</td>\n",
       "      <td>4.057476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.038200</td>\n",
       "      <td>4.056235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 832464675667968\n",
      "Human Readable: 832,464,675,667,968\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.996950387954712, 'eval_runtime': 3.2961, 'eval_samples_per_second': 72.207, 'eval_steps_per_second': 36.103, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 256, n_head = 4, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 16029120, with commas 16,029,120\n",
      "NUMBER OF TOKENS: 35624960, with commas 35,624,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17395' max='17395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17395/17395 47:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.984400</td>\n",
       "      <td>5.439550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.209500</td>\n",
       "      <td>5.141995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.006600</td>\n",
       "      <td>4.971425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.885900</td>\n",
       "      <td>4.854750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.795700</td>\n",
       "      <td>4.794035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.707500</td>\n",
       "      <td>4.697477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.610100</td>\n",
       "      <td>4.607020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.541200</td>\n",
       "      <td>4.522990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.468200</td>\n",
       "      <td>4.428399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.380500</td>\n",
       "      <td>4.349230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.298400</td>\n",
       "      <td>4.272038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.217200</td>\n",
       "      <td>4.199662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>4.131761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.102500</td>\n",
       "      <td>4.064461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.055600</td>\n",
       "      <td>4.037722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.000800</td>\n",
       "      <td>3.973632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.942400</td>\n",
       "      <td>3.937718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.922100</td>\n",
       "      <td>3.903038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.886800</td>\n",
       "      <td>3.869648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.849700</td>\n",
       "      <td>3.840024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.822400</td>\n",
       "      <td>3.817307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.803300</td>\n",
       "      <td>3.784608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.764600</td>\n",
       "      <td>3.763017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.737400</td>\n",
       "      <td>3.743133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.744300</td>\n",
       "      <td>3.722451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.727500</td>\n",
       "      <td>3.707827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.700900</td>\n",
       "      <td>3.692480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.708200</td>\n",
       "      <td>3.680235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.688100</td>\n",
       "      <td>3.671085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.672000</td>\n",
       "      <td>3.661979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.666500</td>\n",
       "      <td>3.656528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.666600</td>\n",
       "      <td>3.652829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.657500</td>\n",
       "      <td>3.650912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.651300</td>\n",
       "      <td>3.649998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3426220553011200\n",
      "Human Readable: 3,426,220,553,011,200\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.617107391357422, 'eval_runtime': 5.4826, 'eval_samples_per_second': 43.41, 'eval_steps_per_second': 21.705, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 320, n_head = 5, gpt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 layers\n",
      "NON EMBEDDING PARAMETERS: 45872064, with commas 45,872,064\n",
      "NUMBER OF TOKENS: 77564928, with commas 77,564,928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37874' max='37874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37874/37874 3:13:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.031200</td>\n",
       "      <td>5.466364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.266000</td>\n",
       "      <td>5.190466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.057200</td>\n",
       "      <td>5.023293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.956300</td>\n",
       "      <td>4.945972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.876700</td>\n",
       "      <td>4.826606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.765500</td>\n",
       "      <td>4.740273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.697000</td>\n",
       "      <td>4.677341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.633700</td>\n",
       "      <td>4.622077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.576200</td>\n",
       "      <td>4.559217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.496700</td>\n",
       "      <td>4.482879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.422400</td>\n",
       "      <td>4.392384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.337600</td>\n",
       "      <td>4.318099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.261900</td>\n",
       "      <td>4.227516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.190200</td>\n",
       "      <td>4.164449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.129900</td>\n",
       "      <td>4.111277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.064000</td>\n",
       "      <td>4.042594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.013800</td>\n",
       "      <td>4.008232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.979400</td>\n",
       "      <td>3.946411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.935200</td>\n",
       "      <td>3.904753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.874500</td>\n",
       "      <td>3.864056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.869000</td>\n",
       "      <td>3.833820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.817700</td>\n",
       "      <td>3.811317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.801900</td>\n",
       "      <td>3.776865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.765500</td>\n",
       "      <td>3.757130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.744700</td>\n",
       "      <td>3.723871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.713200</td>\n",
       "      <td>3.698801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.704400</td>\n",
       "      <td>3.671619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.666000</td>\n",
       "      <td>3.658937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.656400</td>\n",
       "      <td>3.630566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.642700</td>\n",
       "      <td>3.610764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>3.593423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.588100</td>\n",
       "      <td>3.575385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.582500</td>\n",
       "      <td>3.557119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.567900</td>\n",
       "      <td>3.548158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.560600</td>\n",
       "      <td>3.526461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.539900</td>\n",
       "      <td>3.509079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.509500</td>\n",
       "      <td>3.496561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.483500</td>\n",
       "      <td>3.476451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.512000</td>\n",
       "      <td>3.465210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.476600</td>\n",
       "      <td>3.456838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.465200</td>\n",
       "      <td>3.439033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.451200</td>\n",
       "      <td>3.428487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.420300</td>\n",
       "      <td>3.414819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.434800</td>\n",
       "      <td>3.405709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.432900</td>\n",
       "      <td>3.398905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.416900</td>\n",
       "      <td>3.386419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.399400</td>\n",
       "      <td>3.379016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.410700</td>\n",
       "      <td>3.365874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.390200</td>\n",
       "      <td>3.355592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.374300</td>\n",
       "      <td>3.346776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.350300</td>\n",
       "      <td>3.336448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.351100</td>\n",
       "      <td>3.330409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.344500</td>\n",
       "      <td>3.323724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.331900</td>\n",
       "      <td>3.315015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.339600</td>\n",
       "      <td>3.307005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.331500</td>\n",
       "      <td>3.298585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.307800</td>\n",
       "      <td>3.293266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.314400</td>\n",
       "      <td>3.288413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.315800</td>\n",
       "      <td>3.282645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.297300</td>\n",
       "      <td>3.277511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.313600</td>\n",
       "      <td>3.272749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.299000</td>\n",
       "      <td>3.268836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.295300</td>\n",
       "      <td>3.265192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.286600</td>\n",
       "      <td>3.260826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.287600</td>\n",
       "      <td>3.257252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.270400</td>\n",
       "      <td>3.254601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.269800</td>\n",
       "      <td>3.252039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.258300</td>\n",
       "      <td>3.250439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.281200</td>\n",
       "      <td>3.248230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.263900</td>\n",
       "      <td>3.247029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.277000</td>\n",
       "      <td>3.245785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.286500</td>\n",
       "      <td>3.245169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.263700</td>\n",
       "      <td>3.244645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.272700</td>\n",
       "      <td>3.244446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.272300</td>\n",
       "      <td>3.244391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 21348380048228352\n",
      "Human Readable: 21,348,380,048,228,352\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2382781505584717, 'eval_runtime': 10.8531, 'eval_samples_per_second': 21.929, 'eval_steps_per_second': 10.965, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 448, n_head = 7, gpt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEAP TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 49856, with commas 49,856\n",
      "NUMBER OF TOKENS: 497664, with commas 497,664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 148869218304\n",
      "Human Readable: 148,869,218,304\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.114893913269043, 'eval_runtime': 0.8957, 'eval_samples_per_second': 265.72, 'eval_steps_per_second': 132.86, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, n_head = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 198016, with commas 198,016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1380352, with commas 1,380,352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='674' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [674/674 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.107800</td>\n",
       "      <td>5.784122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1639990689792\n",
      "Human Readable: 1,639,990,689,792\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.721367835998535, 'eval_runtime': 0.8224, 'eval_samples_per_second': 289.388, 'eval_steps_per_second': 144.694, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, n_head = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 308960, with commas 308,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\desktop\\leap\\src\\leap\\LEAP.py:200: UserWarning: Using a hidden_size-to-head ratio of greater than 64 is not ideal as LEAP uses a simplified form of attention that relies on having many heads\n",
      "  warnings.warn(\"Using a hidden_size-to-head ratio of greater than 64 is not ideal as\"\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1917952, with commas 1,917,952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='937' max='937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [937/937 00:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.025500</td>\n",
       "      <td>5.665014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3555422699520\n",
      "Human Readable: 3,555,422,699,520\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.601402282714844, 'eval_runtime': 0.8223, 'eval_samples_per_second': 289.432, 'eval_steps_per_second': 144.716, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, n_head = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1776768, with commas 1,776,768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 6995968, with commas 6,995,968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3416' max='3416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3416/3416 02:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.088100</td>\n",
       "      <td>5.521639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.237200</td>\n",
       "      <td>5.147675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.968400</td>\n",
       "      <td>4.943083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.832500</td>\n",
       "      <td>4.823637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.733700</td>\n",
       "      <td>4.751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.679500</td>\n",
       "      <td>4.720037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74581272428544\n",
      "Human Readable: 74,581,272,428,544\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.645600318908691, 'eval_runtime': 1.5258, 'eval_samples_per_second': 155.988, 'eval_steps_per_second': 77.994, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, n_head = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 layers\n",
      "NON EMBEDDING PARAMETERS: 4229344, with commas 4,229,344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 13291520, with commas 13,291,520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6490' max='6490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6490/6490 07:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.155500</td>\n",
       "      <td>5.549298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.256500</td>\n",
       "      <td>5.126582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.980900</td>\n",
       "      <td>4.891197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.780600</td>\n",
       "      <td>4.744344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.655000</td>\n",
       "      <td>4.643302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.566100</td>\n",
       "      <td>4.545161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.485700</td>\n",
       "      <td>4.473008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.426700</td>\n",
       "      <td>4.422051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.375600</td>\n",
       "      <td>4.377839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.333000</td>\n",
       "      <td>4.350260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.311200</td>\n",
       "      <td>4.325484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.304700</td>\n",
       "      <td>4.317365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 337286462177280\n",
      "Human Readable: 337,286,462,177,280\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.252130031585693, 'eval_runtime': 2.175, 'eval_samples_per_second': 109.426, 'eval_steps_per_second': 54.713, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 224, n_head = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n",
      "NON EMBEDDING PARAMETERS: 7099136, with commas 7,099,136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 19499008, with commas 19,499,008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9521' max='9521' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9521/9521 12:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.206400</td>\n",
       "      <td>5.578196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.279000</td>\n",
       "      <td>5.132071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.963200</td>\n",
       "      <td>4.885261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.777000</td>\n",
       "      <td>4.734221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.647800</td>\n",
       "      <td>4.624836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.525700</td>\n",
       "      <td>4.528377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.448600</td>\n",
       "      <td>4.438150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.387900</td>\n",
       "      <td>4.363996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.321800</td>\n",
       "      <td>4.311187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.262100</td>\n",
       "      <td>4.262807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.208100</td>\n",
       "      <td>4.213320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.181500</td>\n",
       "      <td>4.173914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.152800</td>\n",
       "      <td>4.142035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.116600</td>\n",
       "      <td>4.116820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.085100</td>\n",
       "      <td>4.090932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.062700</td>\n",
       "      <td>4.077952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.069900</td>\n",
       "      <td>4.070109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.049700</td>\n",
       "      <td>4.066248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.054400</td>\n",
       "      <td>4.065651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 830556657942528\n",
      "Human Readable: 830,556,657,942,528\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.007962703704834, 'eval_runtime': 2.5397, 'eval_samples_per_second': 93.712, 'eval_steps_per_second': 46.856, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 256, n_head = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n",
      "NON EMBEDDING PARAMETERS: 16012480, with commas 16,012,480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 35598336, with commas 35,598,336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17382' max='17382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17382/17382 31:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.234500</td>\n",
       "      <td>5.631526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.340800</td>\n",
       "      <td>5.209981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.026300</td>\n",
       "      <td>4.926128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.796500</td>\n",
       "      <td>4.755405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.654400</td>\n",
       "      <td>4.624541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.545600</td>\n",
       "      <td>4.527535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.442900</td>\n",
       "      <td>4.407835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.361500</td>\n",
       "      <td>4.329208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.290500</td>\n",
       "      <td>4.259641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.221200</td>\n",
       "      <td>4.212588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.160500</td>\n",
       "      <td>4.154064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.121700</td>\n",
       "      <td>4.108780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.068200</td>\n",
       "      <td>4.061160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.035200</td>\n",
       "      <td>4.023276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.001700</td>\n",
       "      <td>3.987423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.963800</td>\n",
       "      <td>3.963372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.942000</td>\n",
       "      <td>3.930539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.913100</td>\n",
       "      <td>3.905960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.890100</td>\n",
       "      <td>3.882147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.875300</td>\n",
       "      <td>3.854211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.841900</td>\n",
       "      <td>3.836088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.830000</td>\n",
       "      <td>3.816982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.798400</td>\n",
       "      <td>3.799255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.781500</td>\n",
       "      <td>3.785859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.789600</td>\n",
       "      <td>3.772680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.772100</td>\n",
       "      <td>3.760230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.775800</td>\n",
       "      <td>3.747664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.755300</td>\n",
       "      <td>3.737905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.737700</td>\n",
       "      <td>3.730442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.716800</td>\n",
       "      <td>3.725642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.746700</td>\n",
       "      <td>3.720465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.707400</td>\n",
       "      <td>3.717974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.721600</td>\n",
       "      <td>3.716665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.710700</td>\n",
       "      <td>3.716166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 3420105859399680\n",
      "Human Readable: 3,420,105,859,399,680\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.68304181098938, 'eval_runtime': 3.6836, 'eval_samples_per_second': 64.61, 'eval_steps_per_second': 32.305, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 320, n_head = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON EMBEDDING PARAMETERS: 45838016, with commas 45,838,016\n",
      "NUMBER OF TOKENS: 77522944, with commas 77,522,944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37853' max='37853' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37853/37853 2:03:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.327500</td>\n",
       "      <td>5.716834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.448100</td>\n",
       "      <td>5.326819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.160700</td>\n",
       "      <td>5.080652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.992200</td>\n",
       "      <td>4.923663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.814300</td>\n",
       "      <td>4.772987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.679600</td>\n",
       "      <td>4.605647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.544300</td>\n",
       "      <td>4.492287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.442400</td>\n",
       "      <td>4.409285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.360400</td>\n",
       "      <td>4.323043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.276600</td>\n",
       "      <td>4.250167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.202500</td>\n",
       "      <td>4.186943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.161100</td>\n",
       "      <td>4.127966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.112100</td>\n",
       "      <td>4.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.064800</td>\n",
       "      <td>4.033954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.011700</td>\n",
       "      <td>3.988789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.000500</td>\n",
       "      <td>3.959417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.963400</td>\n",
       "      <td>3.939453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.928900</td>\n",
       "      <td>3.900642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.891500</td>\n",
       "      <td>3.870284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.870300</td>\n",
       "      <td>3.853962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.834800</td>\n",
       "      <td>3.812873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.805300</td>\n",
       "      <td>3.805169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.797700</td>\n",
       "      <td>3.770089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.774600</td>\n",
       "      <td>3.750930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.749800</td>\n",
       "      <td>3.744701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.724700</td>\n",
       "      <td>3.722931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.727600</td>\n",
       "      <td>3.702272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>3.680955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.693600</td>\n",
       "      <td>3.672913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.657400</td>\n",
       "      <td>3.646818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.654100</td>\n",
       "      <td>3.638345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.633000</td>\n",
       "      <td>3.622716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>3.609097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.613500</td>\n",
       "      <td>3.589399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.598600</td>\n",
       "      <td>3.585317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.604100</td>\n",
       "      <td>3.568221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.583200</td>\n",
       "      <td>3.557877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.556500</td>\n",
       "      <td>3.547138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.556200</td>\n",
       "      <td>3.534039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.541500</td>\n",
       "      <td>3.525890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.550500</td>\n",
       "      <td>3.511067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.517500</td>\n",
       "      <td>3.509012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.524200</td>\n",
       "      <td>3.496013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.516400</td>\n",
       "      <td>3.485514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.502400</td>\n",
       "      <td>3.470456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.467600</td>\n",
       "      <td>3.463616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.475800</td>\n",
       "      <td>3.452965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.469900</td>\n",
       "      <td>3.446196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.453600</td>\n",
       "      <td>3.436713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.466000</td>\n",
       "      <td>3.428903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.450600</td>\n",
       "      <td>3.421304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.439700</td>\n",
       "      <td>3.415229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.440200</td>\n",
       "      <td>3.412559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.426600</td>\n",
       "      <td>3.405193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.414200</td>\n",
       "      <td>3.397887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.412900</td>\n",
       "      <td>3.391703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.413900</td>\n",
       "      <td>3.387043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.386400</td>\n",
       "      <td>3.381184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.404300</td>\n",
       "      <td>3.375121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.388500</td>\n",
       "      <td>3.369981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.397700</td>\n",
       "      <td>3.366617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.392800</td>\n",
       "      <td>3.363693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.403300</td>\n",
       "      <td>3.359738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.386100</td>\n",
       "      <td>3.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.374200</td>\n",
       "      <td>3.352945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.370900</td>\n",
       "      <td>3.349807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.367100</td>\n",
       "      <td>3.348014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>3.346146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.368000</td>\n",
       "      <td>3.344545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.358700</td>\n",
       "      <td>3.343566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.356800</td>\n",
       "      <td>3.342868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.357500</td>\n",
       "      <td>3.342073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.352400</td>\n",
       "      <td>3.341641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.363200</td>\n",
       "      <td>3.341480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.372000</td>\n",
       "      <td>3.341429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 21320987684634624\n",
      "Human Readable: 21,320,987,684,634,624\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.329268217086792, 'eval_runtime': 6.8597, 'eval_samples_per_second': 34.695, 'eval_steps_per_second': 17.348, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 448, n_head = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 22 layers\n",
      "NON EMBEDDING PARAMETERS: 69308416, with commas 69,308,416\n",
      "NUMBER OF TOKENS: 102674432, with commas 102,674,432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50134' max='50134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50134/50134 3:31:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.327900</td>\n",
       "      <td>5.709722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.474200</td>\n",
       "      <td>5.348254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.168500</td>\n",
       "      <td>5.094281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.998900</td>\n",
       "      <td>5.005149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.891400</td>\n",
       "      <td>4.878734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.770500</td>\n",
       "      <td>4.716152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.637200</td>\n",
       "      <td>4.609263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.519500</td>\n",
       "      <td>4.470326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.426500</td>\n",
       "      <td>4.394495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.345100</td>\n",
       "      <td>4.320373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.271700</td>\n",
       "      <td>4.238564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.193700</td>\n",
       "      <td>4.159550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.141500</td>\n",
       "      <td>4.104445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.073200</td>\n",
       "      <td>4.075042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.024800</td>\n",
       "      <td>4.019669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.012200</td>\n",
       "      <td>3.996829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.966200</td>\n",
       "      <td>3.946470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.917700</td>\n",
       "      <td>3.901366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.900100</td>\n",
       "      <td>3.886983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.900300</td>\n",
       "      <td>3.851015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.838700</td>\n",
       "      <td>3.836498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.830300</td>\n",
       "      <td>3.805321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.815700</td>\n",
       "      <td>3.785225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.775500</td>\n",
       "      <td>3.764884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.771400</td>\n",
       "      <td>3.746971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.743700</td>\n",
       "      <td>3.725244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.734100</td>\n",
       "      <td>3.710228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.692600</td>\n",
       "      <td>3.684728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>3.682437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.665000</td>\n",
       "      <td>3.660979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.645900</td>\n",
       "      <td>3.637091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.636200</td>\n",
       "      <td>3.626367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.619100</td>\n",
       "      <td>3.610645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.612000</td>\n",
       "      <td>3.602980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.632600</td>\n",
       "      <td>3.591956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>3.574160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.593900</td>\n",
       "      <td>3.569230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.581800</td>\n",
       "      <td>3.559101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.561600</td>\n",
       "      <td>3.560237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.565600</td>\n",
       "      <td>3.538358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.555800</td>\n",
       "      <td>3.524550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.535000</td>\n",
       "      <td>3.511157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.538000</td>\n",
       "      <td>3.501019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.523200</td>\n",
       "      <td>3.488410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.496000</td>\n",
       "      <td>3.480106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.494300</td>\n",
       "      <td>3.468717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.468100</td>\n",
       "      <td>3.460431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.468900</td>\n",
       "      <td>3.451229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.477200</td>\n",
       "      <td>3.444416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.468000</td>\n",
       "      <td>3.434717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.437300</td>\n",
       "      <td>3.424021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.449600</td>\n",
       "      <td>3.414134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.428500</td>\n",
       "      <td>3.415330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.426700</td>\n",
       "      <td>3.400129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.417600</td>\n",
       "      <td>3.398633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.420200</td>\n",
       "      <td>3.393151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.416000</td>\n",
       "      <td>3.382264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.399900</td>\n",
       "      <td>3.374866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.376700</td>\n",
       "      <td>3.366565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.394500</td>\n",
       "      <td>3.361937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.364700</td>\n",
       "      <td>3.356812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.388500</td>\n",
       "      <td>3.347431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.372500</td>\n",
       "      <td>3.342456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.360900</td>\n",
       "      <td>3.335491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.352800</td>\n",
       "      <td>3.328044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.349700</td>\n",
       "      <td>3.326084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.346800</td>\n",
       "      <td>3.318103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.337900</td>\n",
       "      <td>3.309631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.336800</td>\n",
       "      <td>3.303457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.327200</td>\n",
       "      <td>3.296417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.324600</td>\n",
       "      <td>3.292793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.313800</td>\n",
       "      <td>3.288776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.309900</td>\n",
       "      <td>3.283687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.309500</td>\n",
       "      <td>3.281479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.308600</td>\n",
       "      <td>3.275953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.299100</td>\n",
       "      <td>3.273052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.300100</td>\n",
       "      <td>3.269046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.288000</td>\n",
       "      <td>3.263762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.301200</td>\n",
       "      <td>3.260211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.297300</td>\n",
       "      <td>3.259489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.270400</td>\n",
       "      <td>3.256735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.275300</td>\n",
       "      <td>3.252541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.288900</td>\n",
       "      <td>3.249302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.270500</td>\n",
       "      <td>3.247640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.287200</td>\n",
       "      <td>3.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.286500</td>\n",
       "      <td>3.243650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.269200</td>\n",
       "      <td>3.241821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.248800</td>\n",
       "      <td>3.239884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.271500</td>\n",
       "      <td>3.238567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.260100</td>\n",
       "      <td>3.237079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.265000</td>\n",
       "      <td>3.235532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.279200</td>\n",
       "      <td>3.235108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.252100</td>\n",
       "      <td>3.234111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.254300</td>\n",
       "      <td>3.233676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.262100</td>\n",
       "      <td>3.232989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.271100</td>\n",
       "      <td>3.232745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.266900</td>\n",
       "      <td>3.232452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.255200</td>\n",
       "      <td>3.232282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.247400</td>\n",
       "      <td>3.232229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.279900</td>\n",
       "      <td>3.232212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 42697213473718272\n",
      "Human Readable: 42,697,213,473,718,272\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2232205867767334, 'eval_runtime': 8.8866, 'eval_samples_per_second': 26.782, 'eval_steps_per_second': 13.391, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 512, n_head = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105268829"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# to count tokens, comes from https://huggingface.co/docs/tokenizers/components\n",
    "whitespace_regex = re.compile(\"\\w+|[^\\w\\s]+\")\n",
    "\n",
    "# get number of tokens\n",
    "total_tokens = 0\n",
    "for row in raw_datasets[\"train\"][\"text\"]:\n",
    "    total_tokens += len((whitespace_regex.split(row)))\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 33408, with commas 33,408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 369664, with commas 369,664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='181' max='181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [181/181 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 74098409472\n",
      "Human Readable: 74,098,409,472\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.131314277648926, 'eval_runtime': 3.2103, 'eval_samples_per_second': 74.137, 'eval_steps_per_second': 37.068, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 64, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 74688, with commas 74,688\n",
      "NUMBER OF TOKENS: 670720, with commas 670,720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='328' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [328/328 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 300568412160\n",
      "Human Readable: 300,568,412,160\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.494235038757324, 'eval_runtime': 3.5429, 'eval_samples_per_second': 67.177, 'eval_steps_per_second': 33.589, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 96, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 132352, with commas 132,352\n",
      "NUMBER OF TOKENS: 1024000, with commas 1,024,000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.526600</td>\n",
       "      <td>8.492445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 813170688000\n",
      "Human Readable: 813,170,688,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.3527250289917, 'eval_runtime': 3.4871, 'eval_samples_per_second': 68.252, 'eval_steps_per_second': 34.126, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 128, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 layers\n",
      "NON EMBEDDING PARAMETERS: 206400, with commas 206,400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 1423360, with commas 1,423,360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='695' max='695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [695/695 00:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.012000</td>\n",
       "      <td>7.334210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 1762689024000\n",
      "Human Readable: 1,762,689,024,000\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.193444728851318, 'eval_runtime': 3.7914, 'eval_samples_per_second': 62.774, 'eval_steps_per_second': 31.387, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 160, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 layers\n",
      "NON EMBEDDING PARAMETERS: 1186176, with commas 1,186,176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TOKENS: 5188608, with commas 5,188,608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2534' max='2534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2534/2534 10:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>14.231200</td>\n",
       "      <td>6.937136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.190400</td>\n",
       "      <td>5.915351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.678500</td>\n",
       "      <td>5.660433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.502000</td>\n",
       "      <td>5.531826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.436800</td>\n",
       "      <td>5.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============TOTAL TRAINING FLOATING POINT OPERATIONS===============\n",
      "\n",
      "Numeric form: 36927613698048\n",
      "Human Readable: 36,927,613,698,048\n",
      "\n",
      "===============TEST SET CROSS ENTROPY LOSS EVALUATION===============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.423725605010986, 'eval_runtime': 12.5413, 'eval_samples_per_second': 18.977, 'eval_steps_per_second': 9.489, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "run_training(hidden_size = 192, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 224, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 256, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 320, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(hidden_size = 448, rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
