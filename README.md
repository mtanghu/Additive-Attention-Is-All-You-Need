# Additive-Attention-Is-Not-All-You-Need-
Additive Attention described by Wu et al. (2021) adapted for causal language modeling. This is a curiosity project about linear attention which preliminary seems to imply that maybe additive attention doesn't quite work for causal language modeling
